import asyncio
import random
from pymilvus import MilvusClient, AsyncMilvusClient, DataType, RRFRanker, AnnSearchRequest
import numpy as np
from scipy.sparse import csr_matrix

# Define the asynchronous function to create a collection
async def create_my_collection(async_client, client, collection_name, schema):
    # Initialize the clients within the coroutine so they use the current event loop.
    
    
    # Check and drop existing collection (synchronous call; ensure it doesn't block if needed)
    if client.has_collection("demo"):
        await client.drop_collection("demo")
    
    # Create collection with the given schema
    await async_client.create_collection(
        collection_name=collection_name,
        schema=schema
    )
    
    if client.has_collection(collection_name):
        print("Collection created successfully")
    else:
        print("Failed to create collection")

# Create schema using async_client functionality (create it inside a coroutine too)
async def create_schema():
    async_client = AsyncMilvusClient()
    client = MilvusClient()
    schema = async_client.create_schema(
        auto_id=False,
        description="sample schema"
    )
    schema.add_field("id", DataType.INT64, is_primary=True)
    schema.add_field("dense_vector", DataType.FLOAT_VECTOR, dim=5)
    schema.add_field("sparse_vector", DataType.SPARSE_FLOAT_VECTOR)
    schema.add_field("text", DataType.VARCHAR, max_length=512)
    return async_client, client, schema

async def create_index(async_client, client , collection_name):
    index_params = client.prepare_index_params()

    index_params.add_index(field_name="dense_vector", index_type="IVF_FLAT", metric_type="IP")
    index_params.add_index(field_name="sparse_vector", index_type="SPARSE_INVERTED_INDEX", metric_type="IP")

    
    await async_client.create_index(collection_name, index_params)
    return True

async def insert_sample_data(async_client, collection_name):
    # Randomly generated data will be used here
    rng = np.random.default_rng(42)

    def generate_random_text(length):
        seed = "this is a seed paragraph to generate random text, which is used for testing purposes. Specifically, a random text is generated by randomly selecting words from this sentence."
        words = seed.split()
        return " ".join(rng.choice(words, length))
    
    data = [{
        'id': i, 
        'dense_vector': rng.random(5).tolist(), 
        'sparse_vector': csr_matrix(rng.random(5)), 
        'text': generate_random_text(10)
    } for i in range(10000)]
    
    await async_client.insert(collection_name, data)
    return True
    
async def query_my_collection(async_client, collection_name):
    res = await async_client.query(
        collection_name = collection_name,
        filter = 'text like "%random%"',
        output_fields = ["count(*)"]
    )
    return res
    
async def conduct_vector_search(async_client, collection_name, type, field):
    rng = np.random.default_rng(42)
    # Generate a set of three random query vectors
    query_vectors = []
    if type == "dense":
        query_vectors = [ rng.random(5) for _ in range(3) ]
    
    if type == "sparse":
        query_vectors = [ csr_matrix(rng.random(5)) for _ in range(3) ]

    print(query_vectors)    
    res = await async_client.search(
        collection_name = collection_name,
        data = query_vectors,
        anns_field = field,
        output_fields= ["text"]
    )
    return res

async def conduct_hybrid_search(async_client, collection_name):
    rng = np.random.default_rng(42)
    
    req_dense = AnnSearchRequest(
        data=[ rng.random(5) for _ in range(3) ],
        anns_field="dense_vector",
        param={"metric_type": "IP"},
        limit=10
    )

    req_sparse = AnnSearchRequest(
        data=[ csr_matrix(rng.random(5)) for _ in range(3) ],
        anns_field="sparse_vector",
        param={"metric_type": "IP"},
        limit=10
    )

    reqs = [req_dense, req_sparse]

    ranker = RRFRanker()

    res = await async_client.hybrid_search(
        collection_name="my_collection",
        reqs=reqs,
        ranker=ranker,
        output_fields=["text", "dense_vector", "sparse_vector"]
    )
    return res
    
async def main():
    collection_name ="my_collection"
    async_client, client, schema = await create_schema()
    await create_my_collection(async_client, client, collection_name, schema)
    await create_index(async_client, client, collection_name)
    await insert_sample_data(async_client, collection_name)
    await async_client.load_collection(collection_name)
    res = await query_my_collection(async_client, collection_name)
    print(res)
    res = await conduct_vector_search(async_client, collection_name, "dense", "dense_vector")
    print(res)
    res = await conduct_hybrid_search(async_client, collection_name)
    print(res)
    await async_client.release_collection(collection_name)
    
    # Optionally drop the collection if it is no longer needed
    await async_client.drop_collection(collection_name)
    
    # Close client connections to free resources
    client.close()
    # If async_client has a close method, then use:
    await async_client.close()

    
    

# Run everything using asyncio.run, which creates a new event loop and runs our main coroutine
asyncio.run(main())
