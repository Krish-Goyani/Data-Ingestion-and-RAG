{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/krishgoyani/Developer/Data Ingestion and RAG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/krishgoyani/Developer/Data Ingestion and RAG\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf4llm in ./env/lib/python3.10/site-packages (0.0.17)\n",
      "Requirement already satisfied: pymupdf>=1.24.10 in ./env/lib/python3.10/site-packages (from pymupdf4llm) (1.25.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pathlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/15==[=====                                   ] ( 2/1=[========                                ] ( 3/15==[==========                              ] ( 4/15==[=============                           ] ( 5/1=[================                        ] ( 6/15==[==================                      ] ( 7/15==[=====================                   ] ( 8/1=[========================                ] ( 9/15==[==========================              ] (10/15==[=============================           ] (11/1=[================================        ] (12/15==[==================================      ] (13/15==[=====================================   ] (14/1=[========================================] (15/15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "md_text = pymupdf4llm.to_markdown(\"file.pdf\")\n",
    "\n",
    "import pathlib\n",
    "pathlib.Path(\"output.md\").write_bytes(md_text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install markitdown -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "md = MarkItDown() # Set to True to enable plugins\n",
    "result = md.convert(\"file.pdf\")\n",
    "with open(\"output.md\", \"w\", encoding=\"utf-8\") as md_file:\n",
    "    md_file.write(result.text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install docling -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    " # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source=\"file.pdf\")\n",
    "\n",
    "with open(\"output.md\", \"w\", encoding=\"utf-8\") as md_file:\n",
    "    md_file.write(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install marker-pdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "from marker.config.parser import ConfigParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model datalab-to/surya_layout on device mps with dtype torch.float16\n",
      "Loaded texify model datalab-to/texify on device mps with dtype torch.float16\n",
      "Loaded recognition model vikp/surya_rec2 on device mps with dtype torch.float16\n",
      "Loaded table recognition model datalab-to/surya_tablerec on device mps with dtype torch.float16\n",
      "Loaded detection model vikp/surya_det3 on device mps with dtype torch.float16\n",
      "Loaded detection model datalab-to/inline_math_det0 on device mps with dtype torch.float16\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"output_format\": \"markdown\",\n",
    "    \"debug\": True,\n",
    "    \"languages\" : \"en\",\n",
    "    #\"gemini_api_key\" : \"AIzaSyCT6izEBAqpzNYFfhf9cZ5nqGYZnpFrG2k\",\n",
    "    #\"use_llm\" : True\n",
    "}\n",
    "\n",
    "config_parser = ConfigParser(config)\n",
    "\n",
    "converter = PdfConverter(\n",
    "    artifact_dict=create_model_dict(),\n",
    "    config= config_parser.generate_config_dict()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|██████████| 3/3 [00:06<00:00,  2.23s/it]\n",
      "LLM layout relabelling: 1it [00:03,  3.06s/it]\n",
      "Running OCR Error Detection: 100%|██████████| 4/4 [00:00<00:00,  8.52it/s]\n",
      "Detecting bboxes: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n",
      "Detecting bboxes: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n",
      "Texify inference: 100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:05<00:00,  5.64s/it]\n",
      "LLMTableProcessor running: 4it [00:13,  3.26s/it]\n",
      "LLMTableMergeProcessor running: 0it [00:00, ?it/s]\n",
      "LLM processors running: 100%|██████████| 6/6 [00:09<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped layout debug images to /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/conversion_results\n",
      "Dumped PDF debug images to /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/conversion_results\n",
      "Dumped block debug data to /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/conversion_results\n"
     ]
    }
   ],
   "source": [
    "rendered = converter(\"file.pdf\")\n",
    "text, metadata, images = text_from_rendered(rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'md'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.md\", \"w\", encoding=\"utf-8\") as md_file:\n",
    "    md_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: _page_2_Figure_0.jpeg\n",
      "Displaying: _page_3_Figure_0.jpeg\n",
      "Displaying: _page_12_Figure_1.jpeg\n",
      "Displaying: _page_13_Figure_0.jpeg\n",
      "Displaying: _page_14_Figure_0.jpeg\n"
     ]
    }
   ],
   "source": [
    "for name, image in images.items():\n",
    "    print(f\"Displaying: {name}\")\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "[H1] Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani∗ Google Brain avaswani@google.com\n",
      "\n",
      "Llion Jones∗ Google Research llion@google.com noam@google.com\n",
      "\n",
      "Noam Shazeer∗ Google Brain\n",
      "\n",
      "Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\n",
      "\n",
      "Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com\n",
      "\n",
      "Jakob Uszkoreit∗ Google Research usz@google.com\n",
      "\n",
      "Niki Parmar∗ Google Research nikip@google.com\n",
      "\n",
      "Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com\n",
      "\n",
      "--- Chunk 3 ---\n",
      "[H1] Attention Is All You Need > [H3] Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. <sup>∗</sup>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "[H1] Attention Is All You Need > [H3] Abstract\n",
      "\n",
      "ebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. <sup>†</sup>Work performed while at Google Brain. <sup>‡</sup>Work performed while at Google Research.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "[H1] 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [\\[13\\]](#page-10-0) and gated recurrent [\\[7\\]](#page-10-1) neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [\\[35,](#page-11-0) [2,](#page-9-0) [5\\]](#page-10-2). Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [\\[38,](#page-11-1) [24,](#page-10-3) [15\\]](#page-10-4). Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [\\[21\\]](#page-10-5) and conditional computation [\\[32\\]](#page-11-2), while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [\\[2,](#page-9-0) [19\\]](#page-10-6). In all but a few cases [\\[27\\]](#page-11-3), however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "\n",
      "--- Chunk 6 ---\n",
      "[H1] 1 Introduction\n",
      "\n",
      "draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "--- Chunk 7 ---\n",
      "[H1] 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [\\[16\\]](#page-10-7), ByteNet [\\[18\\]](#page-10-8) and ConvS2S [\\[9\\]](#page-10-9), all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [\\[12\\]](#page-10-10). In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section [3.2.](#page-2-0)\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [\\[4,](#page-9-1) [27,](#page-11-3) [28,](#page-11-4) [22\\]](#page-10-11). End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [\\[34\\]](#page-11-5). To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.\n",
      "\n",
      "--- Chunk 8 ---\n",
      "[H1] 2 Background\n",
      "\n",
      "without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [\\[17,](#page-10-12) [18\\]](#page-10-8) and [\\[9\\]](#page-10-9).\n",
      "\n",
      "--- Chunk 9 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [\\[5,](#page-10-2) [2,](#page-9-0) [35\\]](#page-11-0). Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1, ..., y_m)$ of symbols one element at a time. At each step the model is auto-regressive [\\[10\\]](#page-10-13), consuming the previously generated symbols as additional input when generating the next. ![](_page_2_Figure_0.jpeg)\n",
      "\n",
      "<span id=\"page-2-1\"></span>Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure [1,](#page-2-1) respectively.\n",
      "\n",
      "--- Chunk 10 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder: The encoder is composed of a stack of $N = 6$ identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [\\[11\\]](#page-10-14) around each of the two sub-layers, followed by layer normalization [\\[1\\]](#page-9-2). That is, the output of each sub-layer is $\text{LayerNorm}(x + \text{Sublayer}(x))$, where $\text{Sublayer}(x)$ is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\text{model}} = 512$. Decoder: The decoder is also composed of a stack of $N = 6$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "--- Chunk 11 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] <span id=\"page-2-0\"></span>3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "![](_page_3_Figure_0.jpeg)\n",
      "\n",
      "<span id=\"page-3-0\"></span>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "--- Chunk 12 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure [2)](#page-3-0). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the queries and keys of dimension $d_k$, and values of dimension $d_v$. we compute the dot products of the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "\n",
      "$$\text{Attention}(Q, K, V) = \text{softmax}(\n",
      "rac{QK^T}{\\sqrt{d_k}})V \tag{l}$$\n",
      "\n",
      "The two most commonly used attention functions are additive attention [\\[2\\]](#page-9-0), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of $\n",
      "rac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [\\[3\\]](#page-9-3). We suspect that for large values of d, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients [4](#page-3-1). To counteract this effect, we scale the dot products by $\n",
      "rac{1}{\\sqrt{d_k}}$. .\n",
      "\n",
      "--- Chunk 13 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] <span id=\"page-3-2\"></span>3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with $d_{\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional\n",
      "\n",
      "<span id=\"page-3-1\"></span><sup>4</sup>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_i k_i$, has mean 0 and variance $d_k$. output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure [2.](#page-3-0)\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. $egin{aligned} \text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \\dots, \text{head}_h) W^O \\ \text{where } \text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\end{aligned}$$\n",
      "\n",
      "Where the projections are parameter matrices $W_i^Q \\in \\mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \times d_{\text{model}}}$ . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "--- Chunk 14 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [\\[38,](#page-11-1) [2,](#page-9-0) [9\\]](#page-10-9). - The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure [2.](#page-3-0)\n",
      "\n",
      "--- Chunk 15 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. $$\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 \tag{2}$$\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\n",
      "\n",
      "--- Chunk 16 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [\\[30\\]](#page-11-6). In the embedding layers, we multiply those weights by √ dmodel. <span id=\"page-5-0\"></span>Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. | Layer Type                  | Complexity per Layer     | Sequential<br>Operations | Maximum Path Length |\n",
      "|-----------------------------|--------------------------|--------------------------|---------------------|\n",
      "| Self-Attention              | $O(n^2 \\cdot d)$         | $O(1)$                   | $O(1)$              |\n",
      "| Recurrent                   | $O(n \\cdot d^2)$         | $O(n)$                   | $O(n)$              |\n",
      "| Convolutional               | $O(k \\cdot n \\cdot d^2)$ | $O(1)$                   | $O(log_k(n))$       |\n",
      "| Self-Attention (restricted) | $O(r \\cdot n \\cdot d)$   | $O(1)$                   | $O(n/r)$            |\n",
      "\n",
      "--- Chunk 17 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [\\[9\\]](#page-10-9). In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "$$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{\text{model}}})$$\n",
      "\n",
      "$$PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{\text{model}}})$$\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. We also experimented with using learned positional embeddings [\\[9\\]](#page-10-9) instead, and found that the two versions produced nearly identical results (see Table [3](#page-8-0) row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "--- Chunk 18 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations $(x_1, ..., x_n)$ to another sequence of equal length $(z_1, ..., z_n)$, with $x_i$, $z_i \\in \\mathbb{R}^d$, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [\\[12\\]](#page-10-10). Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table [1,](#page-5-0) a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $O(n)$ sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [\\[38\\]](#page-11-1) and byte-pair [\\[31\\]](#page-11-7) representations.\n",
      "\n",
      "--- Chunk 19 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 4 Why Self-Attention\n",
      "\n",
      "nd byte-pair [\\[31\\]](#page-11-7) representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work. A single convolutional layer with kernel width $k < n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_k(n))$ in the case of dilated convolutions [\\[18\\]](#page-10-8), increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [\\[6\\]](#page-10-15), however, decrease the complexity considerably, to $O(k \\cdot n \\cdot d + n \\cdot d^2)$. Even with $k = n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "\n",
      "--- Chunk 20 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "--- Chunk 21 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [\\[3\\]](#page-9-3), which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [\\[38\\]](#page-11-1). Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "\n",
      "--- Chunk 22 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table [3)](#page-8-0), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
      "\n",
      "--- Chunk 23 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] 5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [\\[20\\]](#page-10-16) with eta_1$ = 0.9, eta_2$ = 0.98 and $\\epsilon$ = $10^{-9}$. We varied the learning rate over the course of training, according to the formula:\n",
      "\n",
      "$$lrate = d_{\text{model}}^{-0.5} \\cdot \\min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5}) \tag{3}$$\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n",
      "\n",
      "--- Chunk 24 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] <span id=\"page-6-0\"></span>5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "<span id=\"page-7-0\"></span>\n",
      "\n",
      "| Model                           | BLEU  |       | Training Cost (FLOPs) |                     |\n",
      "|---------------------------------|-------|-------|-----------------------|---------------------|\n",
      "|                                 | EN-DE | EN-FR | EN-DE                 | EN-FR               |\n",
      "| ByteNet [18]                    | 23.75 |       |                       |                     |\n",
      "| Deep-Att + PosUnk [39]          |       | 39.2  |                       | $1.0 \\cdot 10^{20}$ |\n",
      "| GNMT + RL [38]                  | 24.6  | 39.92 | $2.3 \\cdot 10^{19}$   | $1.4 \\cdot 10^{20}$ |\n",
      "| ConvS2S [9]                     | 25.16 | 40.46 | $9.6 \\cdot 10^{18}$   | $1.5 \\cdot 10^{20}$ |\n",
      "| MoE [32]                        | 26.03 | 40.56 | $2.0 \\cdot 10^{19}$   | $1.2 \\cdot 10^{20}$ |\n",
      "| Deep-Att + PosUnk Ensemble [39] |       | 40.4  |                       | $8.0 \\cdot 10^{20}$ |\n",
      "| GNMT + RL Ensemble [38]         | 26.30 | 41.16 | $1.8 \\cdot 10^{20}$   | $1.1 \\cdot 10^{21}$ |\n",
      "| ConvS2S Ensemble [9]            | 26.36 | 41.29 | $7.7 \\cdot 10^{19}$   | $1.2 \\cdot 10^{21}$ |\n",
      "| Transformer (base model)        | 27.3  | 38.1  | $3.3 \\cdot 10^{18}$   |                     |\n",
      "| Transformer (big)               | 28.4  | 41.8  | $2.3 \\cdot 10^{19}$   |                     |\n",
      "\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Residual Dropout We apply dropout [\\[33\\]](#page-11-9) to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop}$ = 0.1.\n",
      "\n",
      "--- Chunk 25 ---\n",
      "[H1] 2 Background > [H2] 3 Model Architecture > [H3] <span id=\"page-6-0\"></span>5.4 Regularization\n",
      "\n",
      "the base model, we use a rate of $P_{drop}$ = 0.1. Label Smoothing During training, we employed label smoothing of value $\\epsilon_{ls}$ = 0.1 [\\[36\\]](#page-11-10). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "--- Chunk 26 ---\n",
      "[H1] 6 Results > [H3] 6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table [2)](#page-7-0) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table [3.](#page-8-0) Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate $P_{drop}$ = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty $\u0007lpha$ = 0.6 [\\[38\\]](#page-11-1). These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [\\[38\\]](#page-11-1). Table [2](#page-7-0) summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU [5](#page-7-1) .\n",
      "\n",
      "--- Chunk 27 ---\n",
      "[H1] 6 Results > [H3] 6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "<span id=\"page-7-1\"></span>5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. <span id=\"page-8-0\"></span>Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "\n",
      "--- Chunk 28 ---\n",
      "[H1] 6 Results > [H3] 6.2 Model Variations\n",
      "\n",
      "d should not be compared to per-word perplexities.\n",
      "\n",
      "--- Chunk 29 ---\n",
      "[H1] 6 Results > [H3] 6.2 Model Variations\n",
      "\n",
      "|      | N                                         | dmodel | dff  | h  | dk  | dv  | Pdrop | \\$\\epsilon_{ls}\\$ | train<br>steps | PPL<br>(dev) | BLEU<br>(dev) | params<br>\\$\times10^6\\$ |\n",
      "|------|-------------------------------------------|--------|------|----|-----|-----|-------|--------------------|----------------|--------------|---------------|--------------------------|\n",
      "| base | 6                                         | 512    | 2048 | 8  | 64  | 64  | 0.1   | 0.1                | 100K           | 4.92         | 25.8          | 65                       |\n",
      "| (A)  |                                           |        |      | 1  | 512 | 512 |       |                    |                | 5.29         | 24.9          |                          |\n",
      "|      |                                           |        |      | 4  | 128 | 128 |       |                    |                | 5.00         | 25.5          |                          |\n",
      "|      |                                           |        |      | 16 | 32  | 32  |       |                    |                | 4.91         | 25.8          |                          |\n",
      "|      |                                           |        |      | 32 | 16  | 16  |       |                    |                | 5.01         | 25.4          |                          |\n",
      "| (B)  |                                           |        |      |    | 16  |     |       |                    |                | 5.16         | 25.1          | 58                       |\n",
      "|      |                                           |        |      |    | 32  |     |       |                    |                | 5.01         | 25.4          | 60                       |\n",
      "| (C)  | 2                                         |        |      |    |     |     |       |                    |                | 6.11         | 23.7          | 36                       |\n",
      "|      | 4                                         |      \n",
      "\n",
      "--- Chunk 30 ---\n",
      "[H1] 6 Results > [H3] 6.2 Model Variations\n",
      "\n",
      " 4                                         |      \n",
      "\n",
      "--- Chunk 31 ---\n",
      "[H1] 6 Results > [H3] 6.2 Model Variations\n",
      "\n",
      "  |      |    |     |     |       |                    |                | 5.19         | 25.3          | 50                       |\n",
      "|      | 8                                         |        |      |    |     |     |       |                    |                | 4.88         | 25.5          | 80                       |\n",
      "|      |                                           | 256    |      |    | 32  | 32  |       |                    |                | 5.75         | 24.5          | 28                       |\n",
      "|      |                                           | 1024   |      |    | 128 | 128 |       |                    |                | 4.66         | 26.0          | 168                      |\n",
      "|      |                                           |        | 1024 |    |     |     |       |                    |                | 5.12         | 25.4          | 53                       |\n",
      "|      |                                           |        | 4096 |    |     |     |       |                    |                | 4.75         | 26.2          | 90                       |\n",
      "| (D)  |                                           |        |      |    |     |     | 0.0   |                    |                | 5.77         | 24.6          |                          |\n",
      "|      |                                           |        |      |    |     |     | 0.2   |                    |                | 4.95         | 25.5          |                          |\n",
      "|      |                                           |        |      |    |     |     |       | 0.0                |                | 4.67         | 25.3          |                          |\n",
      "|      |                                           |        |      |    |     |     |       | 0.2                |                | 5.47         | 25.7          |                          |\n",
      "| (E)  | positional embedding instead of sinusoids |        |      |    |     |     |       |                    |\n",
      "\n",
      "--- Chunk 32 ---\n",
      "[H1] 6 Results > [H3] 6.2 Model Variations\n",
      "\n",
      "   |    |     |     |       |                    |                 | 4.92         | 25.7          |                          |\n",
      "| big  | 6                                         | 1024   | 4096 | 16 |     |     | 0.3   |                    | 300K           | 4.33         | 26.4          | 213                      |\n",
      "\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table [3.](#page-8-0)\n",
      "\n",
      "In Table [3](#page-8-0) rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section [3.2.2.](#page-3-2) While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table [3](#page-8-0) rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [\\[9\\]](#page-10-9), and observe nearly identical results to the base model.\n",
      "\n",
      "--- Chunk 33 ---\n",
      "[H1] 6 Results > [H3] 6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [\\[37\\]](#page-11-11). We trained a 4-layer transformer with $d_{model} = 1024$ on the Wall Street Journal (WSJ) portion of the Penn Treebank [\\[25\\]](#page-11-12), about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [\\[37\\]](#page-11-11). We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section [5.4)](#page-6-0), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "\n",
      "<span id=\"page-9-4\"></span>\n",
      "\n",
      "| Parser                              | Training                 | WSJ 23 F1 |\n",
      "|-------------------------------------|--------------------------|-----------|\n",
      "| Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3      |\n",
      "| Petrov et al. (2006) [29]           | WSJ only, discriminative | 90.4      |\n",
      "| Zhu et al. (2013) [40]              | WSJ only, discriminative | 90.4      |\n",
      "| Dyer et al. (2016) [8]              | WSJ only, discriminative | 91.7      |\n",
      "| Transformer (4 layers)              | WSJ only, discriminative | 91.3      |\n",
      "| Zhu et al. (2013) [40]              | semi-supervised          | 91.3      |\n",
      "| Huang & Harper (2009) [14]          | semi-supervised          | 91.3      |\n",
      "| McClosky et al.\n",
      "\n",
      "--- Chunk 34 ---\n",
      "[H1] 6 Results > [H3] 6.3 English Constituency Parsing\n",
      "\n",
      "upervised          | 91.3      |\n",
      "| McClosky et al. (2006) [26]         | semi-supervised          | 92.1      |\n",
      "| Vinyals & Kaiser el al. (2014) [37] | semi-supervised          | 92.1      |\n",
      "| Transformer (4 layers)              | semi-supervised          | 92.7      |\n",
      "| Luong et al. (2015) [23]            | multi-task               | 93.0      |\n",
      "| Dyer et al. (2016) [8]              | generative               | 93.3      |\n",
      "\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and $\u0007lpha = 0.3 $for both WSJ only and the semi-supervised setting. Our results in Table [4](#page-9-4) show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [\\[8\\]](#page-10-17). In contrast to RNN sequence-to-sequence models [\\[37\\]](#page-11-11), the Transformer outperforms the Berkeley-Parser [\\[29\\]](#page-11-13) even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "--- Chunk 35 ---\n",
      "[H1] 6 Results > [H3] 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at [https://github.com/](https://github.com/tensorflow/tensor2tensor) [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor). Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "\n",
      "--- Chunk 36 ---\n",
      "[H1] 6 Results > [H3] References\n",
      "\n",
      "- <span id=\"page-9-2\"></span>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. *arXiv preprint [arXiv:1607.06450](http://arxiv.org/abs/1607.06450)*, 2016. - <span id=\"page-9-0\"></span>[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *CoRR*, abs/1409.0473, 2014. - <span id=\"page-9-3\"></span>[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. *CoRR*, abs/1703.03906, 2017. - <span id=\"page-9-1\"></span>[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. *arXiv preprint [arXiv:1601.06733](http://arxiv.org/abs/1601.06733)*, 2016. - <span id=\"page-10-2\"></span>[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. *CoRR*, abs/1406.1078, 2014. - <span id=\"page-10-15\"></span>[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. *arXiv preprint [arXiv:1610.02357](http://arxiv.org/abs/1610.02357)*, 2016. - <span id=\"page-10-1\"></span>[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. *CoRR*, abs/1412.3555, 2014. - <span id=\"page-10-17\"></span>[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In *Proc. of NAACL*, 2016. - <span id=\"page-10-9\"></span>[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. *arXiv preprint [arXiv:1705.03122v](http://arxiv.org/abs/1705.03122)2*, 2017. - <span id=\"page-10-13\"></span>[10] Alex Graves. Generating sequences with recurrent neural networks.\n",
      "\n",
      "--- Chunk 37 ---\n",
      "[H1] 6 Results > [H3] References\n",
      "\n",
      "nerating sequences with recurrent neural networks. *arXiv preprint [arXiv:1308.0850](http://arxiv.org/abs/1308.0850)*, 2013. - <span id=\"page-10-14\"></span>[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016. - <span id=\"page-10-10\"></span>[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. - <span id=\"page-10-0\"></span>[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. *Neural computation*, 9(8):1735–1780, 1997. - <span id=\"page-10-18\"></span>[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In *Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing*, pages 832–841. ACL, August 2009. - <span id=\"page-10-4\"></span>[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. *arXiv preprint [arXiv:1602.02410](http://arxiv.org/abs/1602.02410)*, 2016. - <span id=\"page-10-7\"></span>[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In *Advances in Neural Information Processing Systems, (NIPS)*, 2016. - <span id=\"page-10-12\"></span>[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In *International Conference on Learning Representations (ICLR)*, 2016. - <span id=\"page-10-8\"></span>[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. *arXiv preprint [arXiv:1610.10099v](http://arxiv.org/abs/1610.10099)2*, 2017. - <span id=\"page-10-6\"></span>[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "\n",
      "--- Chunk 38 ---\n",
      "[H1] 6 Results > [H3] References\n",
      "\n",
      " Alexander M. Rush. Structured attention networks. In *International Conference on Learning Representations*, 2017. - <span id=\"page-10-16\"></span>[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *ICLR*, 2015. - <span id=\"page-10-5\"></span>[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. *arXiv preprint [arXiv:1703.10722](http://arxiv.org/abs/1703.10722)*, 2017. - <span id=\"page-10-11\"></span>[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. *arXiv preprint [arXiv:1703.03130](http://arxiv.org/abs/1703.03130)*, 2017. - <span id=\"page-10-19\"></span>[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. *arXiv preprint [arXiv:1511.06114](http://arxiv.org/abs/1511.06114)*, 2015. - <span id=\"page-10-3\"></span>[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. *arXiv preprint [arXiv:1508.04025](http://arxiv.org/abs/1508.04025)*, 2015. - <span id=\"page-11-12\"></span>[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. *Computational linguistics*, 19(2):313–330, 1993. - <span id=\"page-11-15\"></span>[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In *Proceedings of the Human Language Technology Conference of the NAACL, Main Conference*, pages 152–159. ACL, June 2006. - <span id=\"page-11-3\"></span>[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In *Empirical Methods in Natural Language Processing*, 2016. - <span id=\"page-11-4\"></span>[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.\n",
      "\n",
      "--- Chunk 39 ---\n",
      "[H1] 6 Results > [H3] References\n",
      "\n",
      "ep reinforced model for abstractive summarization. *arXiv preprint [arXiv:1705.04304](http://arxiv.org/abs/1705.04304)*, 2017. - <span id=\"page-11-13\"></span>[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In *Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL*, pages 433–440. ACL, July 2006. - <span id=\"page-11-6\"></span>[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. *arXiv preprint [arXiv:1608.05859](http://arxiv.org/abs/1608.05859)*, 2016. - <span id=\"page-11-7\"></span>[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. *arXiv preprint [arXiv:1508.07909](http://arxiv.org/abs/1508.07909)*, 2015. - <span id=\"page-11-2\"></span>[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint [arXiv:1701.06538](http://arxiv.org/abs/1701.06538)*, 2017. - <span id=\"page-11-9\"></span>[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *Journal of Machine Learning Research*, 15(1):1929–1958, 2014. - <span id=\"page-11-5\"></span>[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing Systems 28*, pages 2440–2448. Curran Associates, Inc., 2015. - <span id=\"page-11-0\"></span>[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In *Advances in Neural Information Processing Systems*, pages 3104–3112, 2014.\n",
      "\n",
      "--- Chunk 40 ---\n",
      "[H1] 6 Results > [H3] References\n",
      "\n",
      "mation Processing Systems*, pages 3104–3112, 2014. - <span id=\"page-11-10\"></span>[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. *CoRR*, abs/1512.00567, 2015. - <span id=\"page-11-11\"></span>[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In *Advances in Neural Information Processing Systems*, 2015. - <span id=\"page-11-1\"></span>[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint [arXiv:1609.08144](http://arxiv.org/abs/1609.08144)*, 2016. - <span id=\"page-11-8\"></span>[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. *CoRR*, abs/1606.04199, 2016. - <span id=\"page-11-14\"></span>[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In *Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers)*, pages 434–443. ACL, August 2013.\n",
      "\n",
      "--- Chunk 41 ---\n",
      "[H1] 6 Results > [H3] References > [H4] Attention Visualizations **Input-Input Layer5**\n",
      "\n",
      "![](_page_12_Figure_1.jpeg)\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color. ![](_page_13_Figure_0.jpeg)\n",
      "\n",
      "**Input-Input Layer5**\n",
      "\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word. ![](_page_14_Figure_0.jpeg)\n",
      "\n",
      "**Input-Input Layer5**\n",
      "\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_content_with_header(prefix: str, content: str, max_chunk_size: int, chunk_overlap: int):\n",
    "    \"\"\"\n",
    "    Split a content block (with a header prefix already added) into chunks that do not exceed\n",
    "    max_chunk_size (in characters). This function tries to avoid breaking sentences.\n",
    "    \n",
    "    Parameters:\n",
    "      - prefix: The header path string to prepend to every chunk.\n",
    "      - content: The text content to split.\n",
    "      - max_chunk_size: Maximum total size of a chunk (including the prefix).\n",
    "      - chunk_overlap: Number of characters from the end of the previous chunk to include at the beginning of the next.\n",
    "    \n",
    "    Returns:\n",
    "      - A list of chunk strings.\n",
    "    \"\"\"\n",
    "    allowed_size = max_chunk_size - len(prefix)\n",
    "    # Use a simple regex to split content into sentences.\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence = sentences[i]\n",
    "        # Decide if we can add the sentence to the current chunk.\n",
    "        candidate = sentence if not current_chunk else current_chunk + \" \" + sentence\n",
    "        if len(candidate) <= allowed_size:\n",
    "            current_chunk = candidate\n",
    "            i += 1\n",
    "        else:\n",
    "            # If the current chunk is empty (i.e. single sentence too long), force-split it.\n",
    "            if not current_chunk:\n",
    "                current_chunk = sentence[:allowed_size]\n",
    "                # Update the sentence to the remaining part.\n",
    "                sentences[i] = sentence[allowed_size:]\n",
    "            # Create the chunk by prepending the header path.\n",
    "            chunk_text = prefix + current_chunk\n",
    "            chunks.append(chunk_text)\n",
    "            # Prepare overlap: take the last 'chunk_overlap' characters from current_chunk (if possible)\n",
    "            if chunk_overlap > 0 and len(current_chunk) > chunk_overlap:\n",
    "                current_chunk = current_chunk[-chunk_overlap:]\n",
    "            else:\n",
    "                current_chunk = \"\"\n",
    "    # Append any remaining text as a final chunk.\n",
    "    if current_chunk:\n",
    "        chunk_text = prefix + current_chunk\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def hierarchical_markdown_chunker(markdown_text: str, max_chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "    \"\"\"\n",
    "    Split a markdown document into chunks while preserving the hierarchical header context.\n",
    "    For each content block, the full header path is prepended so that each chunk is self-contained.\n",
    "    \n",
    "    The algorithm:\n",
    "      1. Parses the markdown line-by-line.\n",
    "      2. Uses header lines (starting with '#') to maintain a header stack.\n",
    "      3. When content is encountered, it is accumulated until the next header.\n",
    "      4. The full header path (e.g., \"[H1] Main Topic > [H2] Subtopic\") is prepended to the content,\n",
    "         and then the content is split into chunks (without breaking sentences if possible).\n",
    "    \n",
    "    Parameters:\n",
    "      - markdown_text: The complete markdown document as a string.\n",
    "      - max_chunk_size: Maximum size (in characters) for each chunk (including header path).\n",
    "      - chunk_overlap: Number of overlapping characters between consecutive chunks.\n",
    "    \n",
    "    Returns:\n",
    "      - A list of text chunks.\n",
    "    \"\"\"\n",
    "    lines = markdown_text.splitlines()\n",
    "    header_stack = []  # List of tuples: (header_level, header_text)\n",
    "    current_content_lines = []\n",
    "    chunks = []\n",
    "    \n",
    "    def flush_current_content():\n",
    "        nonlocal current_content_lines, header_stack, chunks\n",
    "        if current_content_lines:\n",
    "            content = \"\\n\".join(current_content_lines).strip()\n",
    "            if content:\n",
    "                # Build the header breadcrumb string from the current header stack.\n",
    "                header_prefix = \" > \".join(f\"[H{level}] {text}\" for level, text in header_stack)\n",
    "                if header_prefix:\n",
    "                    header_prefix += \"\\n\\n\"\n",
    "                # Split the content into chunks, each with the header prefix.\n",
    "                sub_chunks = split_content_with_header(header_prefix, content, max_chunk_size, chunk_overlap)\n",
    "                chunks.extend(sub_chunks)\n",
    "            current_content_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.lstrip()\n",
    "        # Check if the line is a markdown header.\n",
    "        if stripped.startswith(\"#\"):\n",
    "            # Flush any accumulated content before updating the header context.\n",
    "            flush_current_content()\n",
    "            # Determine header level (count of '#' characters)\n",
    "            level = len(stripped) - len(stripped.lstrip(\"#\"))\n",
    "            header_text = stripped[level:].strip()\n",
    "            # Remove headers from the stack that are at the same or deeper level.\n",
    "            while header_stack and header_stack[-1][0] >= level:\n",
    "                header_stack.pop()\n",
    "            # Add the new header to the stack.\n",
    "            header_stack.append((level, header_text))\n",
    "        else:\n",
    "            # Normal content: accumulate the line.\n",
    "            current_content_lines.append(line)\n",
    "    \n",
    "    # Flush any remaining content after processing all lines.\n",
    "    flush_current_content()\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    example_markdown = \"\"\"\n",
    "# Main Topic\n",
    "This is the introduction of the main topic. It sets the stage.\n",
    "\n",
    "## Subtopic A\n",
    "Details about subtopic A. It goes into specifics. The content here is quite detailed and might span several sentences. It is important to understand how the ideas connect.\n",
    "\n",
    "## Subtopic B\n",
    "Information about subtopic B. This section might include multiple paragraphs.\n",
    "Another paragraph that elaborates further.\n",
    "\n",
    "### Subtopic B.1\n",
    "Even more details under subtopic B.1. This chunk is smaller.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Customize your chunk size and overlap as needed.\n",
    "    chunks = hierarchical_markdown_chunker(text, max_chunk_size=2000, chunk_overlap=50)\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks, 1):\n",
    "        print(f\"--- Chunk {idx} ---\")\n",
    "        print(chunk)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
    "\n",
    "# Attention Is All You Need\n",
    "\n",
    "Ashish Vaswani∗ Google Brain avaswani@google.com\n",
    "\n",
    "Llion Jones∗ Google Research llion@google.com noam@google.com\n",
    "\n",
    "Noam Shazeer∗ Google Brain\n",
    "\n",
    "Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\n",
    "\n",
    "Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com\n",
    "\n",
    "Jakob Uszkoreit∗ Google Research usz@google.com\n",
    "\n",
    "Niki Parmar∗ Google Research nikip@google.com\n",
    "\n",
    "Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com\n",
    "\n",
    "### Abstract\n",
    "\n",
    "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
    "\n",
    "<sup>∗</sup>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
    "\n",
    "<sup>†</sup>Work performed while at Google Brain.\n",
    "\n",
    "<sup>‡</sup>Work performed while at Google Research.\n",
    "\n",
    "# 1 Introduction\n",
    "\n",
    "Recurrent neural networks, long short-term memory [\\[13\\]](#page-10-0) and gated recurrent [\\[7\\]](#page-10-1) neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [\\[35,](#page-11-0) [2,](#page-9-0) [5\\]](#page-10-2). Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [\\[38,](#page-11-1) [24,](#page-10-3) [15\\]](#page-10-4).\n",
    "\n",
    "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [\\[21\\]](#page-10-5) and conditional computation [\\[32\\]](#page-11-2), while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
    "\n",
    "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [\\[2,](#page-9-0) [19\\]](#page-10-6). In all but a few cases [\\[27\\]](#page-11-3), however, such attention mechanisms are used in conjunction with a recurrent network.\n",
    "\n",
    "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
    "\n",
    "# 2 Background\n",
    "\n",
    "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [\\[16\\]](#page-10-7), ByteNet [\\[18\\]](#page-10-8) and ConvS2S [\\[9\\]](#page-10-9), all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [\\[12\\]](#page-10-10). In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section [3.2.](#page-2-0)\n",
    "\n",
    "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [\\[4,](#page-9-1) [27,](#page-11-3) [28,](#page-11-4) [22\\]](#page-10-11).\n",
    "\n",
    "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [\\[34\\]](#page-11-5).\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [\\[17,](#page-10-12) [18\\]](#page-10-8) and [\\[9\\]](#page-10-9).\n",
    "\n",
    "## 3 Model Architecture\n",
    "\n",
    "Most competitive neural sequence transduction models have an encoder-decoder structure [\\[5,](#page-10-2) [2,](#page-9-0) [35\\]](#page-11-0). Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1, ..., y_m)$ of symbols one element at a time. At each step the model is auto-regressive [\\[10\\]](#page-10-13), consuming the previously generated symbols as additional input when generating the next.\n",
    "\n",
    "![](_page_2_Figure_0.jpeg)\n",
    "\n",
    "<span id=\"page-2-1\"></span>Figure 1: The Transformer - model architecture.\n",
    "\n",
    "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure [1,](#page-2-1) respectively.\n",
    "\n",
    "### 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "Encoder: The encoder is composed of a stack of $N = 6$ identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [\\[11\\]](#page-10-14) around each of the two sub-layers, followed by layer normalization [\\[1\\]](#page-9-2). That is, the output of each sub-layer is $\\text{LayerNorm}(x + \\text{Sublayer}(x))$, where $\\text{Sublayer}(x)$ is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}} = 512$.\n",
    "\n",
    "Decoder: The decoder is also composed of a stack of $N = 6$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
    "\n",
    "### <span id=\"page-2-0\"></span>3.2 Attention\n",
    "\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
    "\n",
    "![](_page_3_Figure_0.jpeg)\n",
    "\n",
    "<span id=\"page-3-0\"></span>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
    "\n",
    "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "### 3.2.1 Scaled Dot-Product Attention\n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\" (Figure [2)](#page-3-0). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the queries and keys of dimension $d_k$, and values of dimension $d_v$. we compute the dot products of the values.\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\tag{l}$$\n",
    "\n",
    "The two most commonly used attention functions are additive attention [\\[2\\]](#page-9-0), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
    "\n",
    "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [\\[3\\]](#page-9-3). We suspect that for large values of d, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients [4](#page-3-1). To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$. .\n",
    "\n",
    "### <span id=\"page-3-2\"></span>3.2.2 Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function with $d_{\\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional\n",
    "\n",
    "<span id=\"page-3-1\"></span><sup>4</sup>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_i k_i$, has mean 0 and variance $d_k$.\n",
    "\n",
    "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure [2.](#page-3-0)\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$$\\begin{aligned} \\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O \\\\ \\text{where } \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\end{aligned}$$\n",
    "\n",
    "Where the projections are parameter matrices $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$ .\n",
    "\n",
    "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
    "\n",
    "### 3.2.3 Applications of Attention in our Model\n",
    "\n",
    "The Transformer uses multi-head attention in three different ways:\n",
    "\n",
    "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [\\[38,](#page-11-1) [2,](#page-9-0) [9\\]](#page-10-9).\n",
    "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
    "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure [2.](#page-3-0)\n",
    "\n",
    "### 3.3 Position-wise Feed-Forward Networks\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 \\tag{2}$$\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\n",
    "\n",
    "### 3.4 Embeddings and Softmax\n",
    "\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [\\[30\\]](#page-11-6). In the embedding layers, we multiply those weights by √ dmodel.\n",
    "\n",
    "<span id=\"page-5-0\"></span>Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
    "\n",
    "| Layer Type                  | Complexity per Layer     | Sequential<br>Operations | Maximum Path Length |\n",
    "|-----------------------------|--------------------------|--------------------------|---------------------|\n",
    "| Self-Attention              | $O(n^2 \\cdot d)$         | $O(1)$                   | $O(1)$              |\n",
    "| Recurrent                   | $O(n \\cdot d^2)$         | $O(n)$                   | $O(n)$              |\n",
    "| Convolutional               | $O(k \\cdot n \\cdot d^2)$ | $O(1)$                   | $O(log_k(n))$       |\n",
    "| Self-Attention (restricted) | $O(r \\cdot n \\cdot d)$   | $O(1)$                   | $O(n/r)$            |\n",
    "\n",
    "### 3.5 Positional Encoding\n",
    "\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [\\[9\\]](#page-10-9).\n",
    "\n",
    "In this work, we use sine and cosine functions of different frequencies:\n",
    "\n",
    "$$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.\n",
    "\n",
    "We also experimented with using learned positional embeddings [\\[9\\]](#page-10-9) instead, and found that the two versions produced nearly identical results (see Table [3](#page-8-0) row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
    "\n",
    "### 4 Why Self-Attention\n",
    "\n",
    "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations $(x_1, ..., x_n)$ to another sequence of equal length $(z_1, ..., z_n)$, with $x_i$, $z_i \\in \\mathbb{R}^d$, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
    "\n",
    "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
    "\n",
    "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [\\[12\\]](#page-10-10). Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
    "\n",
    "As noted in Table [1,](#page-5-0) a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $O(n)$ sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
    "\n",
    "length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [\\[38\\]](#page-11-1) and byte-pair [\\[31\\]](#page-11-7) representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.\n",
    "\n",
    "A single convolutional layer with kernel width $k < n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_k(n))$ in the case of dilated convolutions [\\[18\\]](#page-10-8), increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [\\[6\\]](#page-10-15), however, decrease the complexity considerably, to $O(k \\cdot n \\cdot d + n \\cdot d^2)$. Even with $k = n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
    "\n",
    "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
    "\n",
    "### 5 Training\n",
    "\n",
    "This section describes the training regime for our models.\n",
    "\n",
    "### 5.1 Training Data and Batching\n",
    "\n",
    "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [\\[3\\]](#page-9-3), which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [\\[38\\]](#page-11-1). Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
    "\n",
    "### 5.2 Hardware and Schedule\n",
    "\n",
    "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table [3)](#page-8-0), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
    "\n",
    "### 5.3 Optimizer\n",
    "\n",
    "We used the Adam optimizer [\\[20\\]](#page-10-16) with $\\beta_1$ = 0.9, $\\beta_2$ = 0.98 and $\\epsilon$ = $10^{-9}$. We varied the learning rate over the course of training, according to the formula:\n",
    "\n",
    "$$lrate = d_{\\text{model}}^{-0.5} \\cdot \\min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5}) \\tag{3}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n",
    "\n",
    "### <span id=\"page-6-0\"></span>5.4 Regularization\n",
    "\n",
    "We employ three types of regularization during training:\n",
    "\n",
    "<span id=\"page-7-0\"></span>\n",
    "\n",
    "| Model                           | BLEU  |       | Training Cost (FLOPs) |                     |\n",
    "|---------------------------------|-------|-------|-----------------------|---------------------|\n",
    "|                                 | EN-DE | EN-FR | EN-DE                 | EN-FR               |\n",
    "| ByteNet [18]                    | 23.75 |       |                       |                     |\n",
    "| Deep-Att + PosUnk [39]          |       | 39.2  |                       | $1.0 \\cdot 10^{20}$ |\n",
    "| GNMT + RL [38]                  | 24.6  | 39.92 | $2.3 \\cdot 10^{19}$   | $1.4 \\cdot 10^{20}$ |\n",
    "| ConvS2S [9]                     | 25.16 | 40.46 | $9.6 \\cdot 10^{18}$   | $1.5 \\cdot 10^{20}$ |\n",
    "| MoE [32]                        | 26.03 | 40.56 | $2.0 \\cdot 10^{19}$   | $1.2 \\cdot 10^{20}$ |\n",
    "| Deep-Att + PosUnk Ensemble [39] |       | 40.4  |                       | $8.0 \\cdot 10^{20}$ |\n",
    "| GNMT + RL Ensemble [38]         | 26.30 | 41.16 | $1.8 \\cdot 10^{20}$   | $1.1 \\cdot 10^{21}$ |\n",
    "| ConvS2S Ensemble [9]            | 26.36 | 41.29 | $7.7 \\cdot 10^{19}$   | $1.2 \\cdot 10^{21}$ |\n",
    "| Transformer (base model)        | 27.3  | 38.1  | $3.3 \\cdot 10^{18}$   |                     |\n",
    "| Transformer (big)               | 28.4  | 41.8  | $2.3 \\cdot 10^{19}$   |                     |\n",
    "\n",
    "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
    "\n",
    "Residual Dropout We apply dropout [\\[33\\]](#page-11-9) to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop}$ = 0.1.\n",
    "\n",
    "Label Smoothing During training, we employed label smoothing of value $\\epsilon_{ls}$ = 0.1 [\\[36\\]](#page-11-10). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
    "\n",
    "# 6 Results\n",
    "\n",
    "### 6.1 Machine Translation\n",
    "\n",
    "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table [2)](#page-7-0) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table [3.](#page-8-0) Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
    "\n",
    "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate $P_{drop}$ = 0.1, instead of 0.3.\n",
    "\n",
    "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty $\\alpha$ = 0.6 [\\[38\\]](#page-11-1). These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [\\[38\\]](#page-11-1).\n",
    "\n",
    "Table [2](#page-7-0) summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU [5](#page-7-1) .\n",
    "\n",
    "### 6.2 Model Variations\n",
    "\n",
    "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n",
    "\n",
    "<span id=\"page-7-1\"></span>5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
    "\n",
    "<span id=\"page-8-0\"></span>Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
    "\n",
    "|      | N                                         | dmodel | dff  | h  | dk  | dv  | Pdrop | \\$\\\\epsilon_{ls}\\$ | train<br>steps | PPL<br>(dev) | BLEU<br>(dev) | params<br>\\$\\times10^6\\$ |\n",
    "|------|-------------------------------------------|--------|------|----|-----|-----|-------|--------------------|----------------|--------------|---------------|--------------------------|\n",
    "| base | 6                                         | 512    | 2048 | 8  | 64  | 64  | 0.1   | 0.1                | 100K           | 4.92         | 25.8          | 65                       |\n",
    "| (A)  |                                           |        |      | 1  | 512 | 512 |       |                    |                | 5.29         | 24.9          |                          |\n",
    "|      |                                           |        |      | 4  | 128 | 128 |       |                    |                | 5.00         | 25.5          |                          |\n",
    "|      |                                           |        |      | 16 | 32  | 32  |       |                    |                | 4.91         | 25.8          |                          |\n",
    "|      |                                           |        |      | 32 | 16  | 16  |       |                    |                | 5.01         | 25.4          |                          |\n",
    "| (B)  |                                           |        |      |    | 16  |     |       |                    |                | 5.16         | 25.1          | 58                       |\n",
    "|      |                                           |        |      |    | 32  |     |       |                    |                | 5.01         | 25.4          | 60                       |\n",
    "| (C)  | 2                                         |        |      |    |     |     |       |                    |                | 6.11         | 23.7          | 36                       |\n",
    "|      | 4                                         |        |      |    |     |     |       |                    |                | 5.19         | 25.3          | 50                       |\n",
    "|      | 8                                         |        |      |    |     |     |       |                    |                | 4.88         | 25.5          | 80                       |\n",
    "|      |                                           | 256    |      |    | 32  | 32  |       |                    |                | 5.75         | 24.5          | 28                       |\n",
    "|      |                                           | 1024   |      |    | 128 | 128 |       |                    |                | 4.66         | 26.0          | 168                      |\n",
    "|      |                                           |        | 1024 |    |     |     |       |                    |                | 5.12         | 25.4          | 53                       |\n",
    "|      |                                           |        | 4096 |    |     |     |       |                    |                | 4.75         | 26.2          | 90                       |\n",
    "| (D)  |                                           |        |      |    |     |     | 0.0   |                    |                | 5.77         | 24.6          |                          |\n",
    "|      |                                           |        |      |    |     |     | 0.2   |                    |                | 4.95         | 25.5          |                          |\n",
    "|      |                                           |        |      |    |     |     |       | 0.0                |                | 4.67         | 25.3          |                          |\n",
    "|      |                                           |        |      |    |     |     |       | 0.2                |                | 5.47         | 25.7          |                          |\n",
    "| (E)  | positional embedding instead of sinusoids |        |      |    |     |     |       |                    |                | 4.92         | 25.7          |                          |\n",
    "| big  | 6                                         | 1024   | 4096 | 16 |     |     | 0.3   |                    | 300K           | 4.33         | 26.4          | 213                      |\n",
    "\n",
    "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table [3.](#page-8-0)\n",
    "\n",
    "In Table [3](#page-8-0) rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section [3.2.2.](#page-3-2) While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
    "\n",
    "In Table [3](#page-8-0) rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [\\[9\\]](#page-10-9), and observe nearly identical results to the base model.\n",
    "\n",
    "### 6.3 English Constituency Parsing\n",
    "\n",
    "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [\\[37\\]](#page-11-11).\n",
    "\n",
    "We trained a 4-layer transformer with $d_{model} = 1024$ on the Wall Street Journal (WSJ) portion of the Penn Treebank [\\[25\\]](#page-11-12), about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [\\[37\\]](#page-11-11). We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
    "\n",
    "We performed only a small number of experiments to select the dropout, both attention and residual (section [5.4)](#page-6-0), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
    "\n",
    "<span id=\"page-9-4\"></span>\n",
    "\n",
    "| Parser                              | Training                 | WSJ 23 F1 |\n",
    "|-------------------------------------|--------------------------|-----------|\n",
    "| Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3      |\n",
    "| Petrov et al. (2006) [29]           | WSJ only, discriminative | 90.4      |\n",
    "| Zhu et al. (2013) [40]              | WSJ only, discriminative | 90.4      |\n",
    "| Dyer et al. (2016) [8]              | WSJ only, discriminative | 91.7      |\n",
    "| Transformer (4 layers)              | WSJ only, discriminative | 91.3      |\n",
    "| Zhu et al. (2013) [40]              | semi-supervised          | 91.3      |\n",
    "| Huang & Harper (2009) [14]          | semi-supervised          | 91.3      |\n",
    "| McClosky et al. (2006) [26]         | semi-supervised          | 92.1      |\n",
    "| Vinyals & Kaiser el al. (2014) [37] | semi-supervised          | 92.1      |\n",
    "| Transformer (4 layers)              | semi-supervised          | 92.7      |\n",
    "| Luong et al. (2015) [23]            | multi-task               | 93.0      |\n",
    "| Dyer et al. (2016) [8]              | generative               | 93.3      |\n",
    "\n",
    "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
    "\n",
    "increased the maximum output length to input length + 300. We used a beam size of 21 and $\\alpha = 0.3 $for both WSJ only and the semi-supervised setting.\n",
    "\n",
    "Our results in Table [4](#page-9-4) show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [\\[8\\]](#page-10-17).\n",
    "\n",
    "In contrast to RNN sequence-to-sequence models [\\[37\\]](#page-11-11), the Transformer outperforms the Berkeley-Parser [\\[29\\]](#page-11-13) even when training only on the WSJ training set of 40K sentences.\n",
    "\n",
    "### 7 Conclusion\n",
    "\n",
    "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
    "\n",
    "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
    "\n",
    "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
    "\n",
    "The code we used to train and evaluate our models is available at [https://github.com/](https://github.com/tensorflow/tensor2tensor) [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor).\n",
    "\n",
    "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
    "\n",
    "### References\n",
    "\n",
    "- <span id=\"page-9-2\"></span>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. *arXiv preprint [arXiv:1607.06450](http://arxiv.org/abs/1607.06450)*, 2016.\n",
    "- <span id=\"page-9-0\"></span>[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *CoRR*, abs/1409.0473, 2014.\n",
    "- <span id=\"page-9-3\"></span>[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. *CoRR*, abs/1703.03906, 2017.\n",
    "- <span id=\"page-9-1\"></span>[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. *arXiv preprint [arXiv:1601.06733](http://arxiv.org/abs/1601.06733)*, 2016.\n",
    "- <span id=\"page-10-2\"></span>[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. *CoRR*, abs/1406.1078, 2014.\n",
    "- <span id=\"page-10-15\"></span>[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. *arXiv preprint [arXiv:1610.02357](http://arxiv.org/abs/1610.02357)*, 2016.\n",
    "- <span id=\"page-10-1\"></span>[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. *CoRR*, abs/1412.3555, 2014.\n",
    "- <span id=\"page-10-17\"></span>[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In *Proc. of NAACL*, 2016.\n",
    "- <span id=\"page-10-9\"></span>[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. *arXiv preprint [arXiv:1705.03122v](http://arxiv.org/abs/1705.03122)2*, 2017.\n",
    "- <span id=\"page-10-13\"></span>[10] Alex Graves. Generating sequences with recurrent neural networks. *arXiv preprint [arXiv:1308.0850](http://arxiv.org/abs/1308.0850)*, 2013.\n",
    "- <span id=\"page-10-14\"></span>[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016.\n",
    "- <span id=\"page-10-10\"></span>[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
    "- <span id=\"page-10-0\"></span>[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. *Neural computation*, 9(8):1735–1780, 1997.\n",
    "- <span id=\"page-10-18\"></span>[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In *Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing*, pages 832–841. ACL, August 2009.\n",
    "- <span id=\"page-10-4\"></span>[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. *arXiv preprint [arXiv:1602.02410](http://arxiv.org/abs/1602.02410)*, 2016.\n",
    "- <span id=\"page-10-7\"></span>[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In *Advances in Neural Information Processing Systems, (NIPS)*, 2016.\n",
    "- <span id=\"page-10-12\"></span>[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In *International Conference on Learning Representations (ICLR)*, 2016.\n",
    "- <span id=\"page-10-8\"></span>[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. *arXiv preprint [arXiv:1610.10099v](http://arxiv.org/abs/1610.10099)2*, 2017.\n",
    "- <span id=\"page-10-6\"></span>[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In *International Conference on Learning Representations*, 2017.\n",
    "- <span id=\"page-10-16\"></span>[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *ICLR*, 2015.\n",
    "- <span id=\"page-10-5\"></span>[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. *arXiv preprint [arXiv:1703.10722](http://arxiv.org/abs/1703.10722)*, 2017.\n",
    "- <span id=\"page-10-11\"></span>[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. *arXiv preprint [arXiv:1703.03130](http://arxiv.org/abs/1703.03130)*, 2017.\n",
    "- <span id=\"page-10-19\"></span>[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. *arXiv preprint [arXiv:1511.06114](http://arxiv.org/abs/1511.06114)*, 2015.\n",
    "- <span id=\"page-10-3\"></span>[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. *arXiv preprint [arXiv:1508.04025](http://arxiv.org/abs/1508.04025)*, 2015.\n",
    "- <span id=\"page-11-12\"></span>[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. *Computational linguistics*, 19(2):313–330, 1993.\n",
    "- <span id=\"page-11-15\"></span>[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In *Proceedings of the Human Language Technology Conference of the NAACL, Main Conference*, pages 152–159. ACL, June 2006.\n",
    "- <span id=\"page-11-3\"></span>[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In *Empirical Methods in Natural Language Processing*, 2016.\n",
    "- <span id=\"page-11-4\"></span>[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. *arXiv preprint [arXiv:1705.04304](http://arxiv.org/abs/1705.04304)*, 2017.\n",
    "- <span id=\"page-11-13\"></span>[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In *Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL*, pages 433–440. ACL, July 2006.\n",
    "- <span id=\"page-11-6\"></span>[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. *arXiv preprint [arXiv:1608.05859](http://arxiv.org/abs/1608.05859)*, 2016.\n",
    "- <span id=\"page-11-7\"></span>[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. *arXiv preprint [arXiv:1508.07909](http://arxiv.org/abs/1508.07909)*, 2015.\n",
    "- <span id=\"page-11-2\"></span>[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint [arXiv:1701.06538](http://arxiv.org/abs/1701.06538)*, 2017.\n",
    "- <span id=\"page-11-9\"></span>[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *Journal of Machine Learning Research*, 15(1):1929–1958, 2014.\n",
    "- <span id=\"page-11-5\"></span>[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing Systems 28*, pages 2440–2448. Curran Associates, Inc., 2015.\n",
    "- <span id=\"page-11-0\"></span>[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In *Advances in Neural Information Processing Systems*, pages 3104–3112, 2014.\n",
    "- <span id=\"page-11-10\"></span>[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. *CoRR*, abs/1512.00567, 2015.\n",
    "- <span id=\"page-11-11\"></span>[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In *Advances in Neural Information Processing Systems*, 2015.\n",
    "- <span id=\"page-11-1\"></span>[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint [arXiv:1609.08144](http://arxiv.org/abs/1609.08144)*, 2016.\n",
    "- <span id=\"page-11-8\"></span>[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. *CoRR*, abs/1606.04199, 2016.\n",
    "- <span id=\"page-11-14\"></span>[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In *Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers)*, pages 434–443. ACL, August 2013.\n",
    "\n",
    "#### Attention Visualizations **Input-Input Layer5**\n",
    "\n",
    "![](_page_12_Figure_1.jpeg)\n",
    "\n",
    "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\n",
    "\n",
    "![](_page_13_Figure_0.jpeg)\n",
    "\n",
    "**Input-Input Layer5**\n",
    "\n",
    "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
    "\n",
    "![](_page_14_Figure_0.jpeg)\n",
    "\n",
    "**Input-Input Layer5**\n",
    "\n",
    "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index -q\n",
    "!pip install llama-index-embeddings-huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"essay.txt\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_text_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 106\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# --- Step 1: Read and Split the Essay ---\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124messay.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 106\u001b[0m     essay \u001b[38;5;241m=\u001b[39m \u001b[43mread_text_file\u001b[49m(filepath)\n\u001b[1;32m    107\u001b[0m     sentence_list \u001b[38;5;241m=\u001b[39m split_into_sentences(essay)\n\u001b[1;32m    108\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m create_sentence_dicts(sentence_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_text_file' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split text into sentences using punctuation markers (., ?, !)\n",
    "    followed by whitespace.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    print(f\"{len(sentences)} sentences were found\")\n",
    "    return sentences\n",
    "\n",
    "def create_sentence_dicts(sentences: list) -> list:\n",
    "    \"\"\"Convert a list of sentences into a list of dictionaries with index.\"\"\"\n",
    "    return [{'sentence': sentence, 'index': i} for i, sentence in enumerate(sentences)]\n",
    "\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def embed_combined_sentences(sentences: list) -> list:\n",
    "    \"\"\"\n",
    "    Generate embeddings for each combined sentence using OpenAI embeddings.\n",
    "    The embeddings are stored under the key 'combined_sentence_embedding'.\n",
    "    \"\"\"\n",
    "   \n",
    "    embeddings = [embed_model.get_text_embedding(s[\"combined_sentence\"]) for s in sentences]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    return sentences\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences\n",
    "\n",
    "def determine_breakpoints(distances: list, threshold: float) -> list:\n",
    "    \"\"\"\n",
    "    Determine indices in the sentences list where the cosine distance exceeds\n",
    "    the threshold, indicating a semantic break.\n",
    "    \"\"\"\n",
    "    return [i for i, d in enumerate(distances) if d > threshold]\n",
    "\n",
    "def group_sentences_into_chunks(sentences: list, breakpoint_indices: list) -> list:\n",
    "    \"\"\"\n",
    "    Group sentences into chunks based on the determined breakpoints.\n",
    "    Each chunk is a concatenation of the original 'sentence' texts.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    for bp in breakpoint_indices:\n",
    "        group = sentences[start_index:bp+1]\n",
    "        combined_text = ' '.join(d['sentence'] for d in group)\n",
    "        chunks.append(combined_text)\n",
    "        start_index = bp + 1\n",
    "    # Handle any remaining sentences as the last chunk.\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join(d['sentence'] for d in sentences[start_index:])\n",
    "        chunks.append(combined_text)\n",
    "    return chunks\n",
    "\n",
    "def main():\n",
    "    # --- Step 1: Read and Split the Essay ---\n",
    "    filepath = 'essay.txt'\n",
    "    essay = read_text_file(filepath)\n",
    "    sentence_list = split_into_sentences(essay)\n",
    "    sentences = create_sentence_dicts(sentence_list)\n",
    "\n",
    "    # --- Step 2: Combine Sentences with Buffer for Context ---\n",
    "    sentences = combine_sentences(sentences, buffer_size=1)\n",
    "\n",
    "    # --- Step 3: Generate Embeddings for the Combined Sentences ---\n",
    "    sentences = embed_combined_sentences(sentences)\n",
    "\n",
    "    # --- Step 4: Calculate Cosine Distances Between Consecutive Embeddings ---\n",
    "    distances, sentences = calculate_cosine_distances(sentences)\n",
    "\n",
    "    # --- Step 5: Determine Breakpoints Based on a Distance Threshold ---\n",
    "    # Adjust threshold as needed (e.g., 0.3 is used here)\n",
    "    threshold = .3\n",
    "    breakpoint_indices = determine_breakpoints(distances, threshold)\n",
    "    print(\"Breakpoint indices:\", breakpoint_indices)\n",
    "\n",
    "    # --- Step 6: Group Original Sentences into Chunks Based on Breakpoints ---\n",
    "    chunks = group_sentences_into_chunks(sentences, breakpoint_indices)\n",
    "    print(f\"length of chunks : {len(chunks)}\")\n",
    "    # --- Step 7: Inspect the First Few Chunks ---\n",
    "    for i, chunk in enumerate(chunks[:2]):\n",
    "        print(f\"Chunk #{i}\")\n",
    "        print(chunk.strip())\n",
    "        print(\"...\")\n",
    "        print(chunk.strip())\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n",
      "\n",
      "I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n",
      "\n",
      "With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\n",
      "\n",
      "The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\n",
      "\n",
      "Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n",
      "\n",
      "Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\n",
      "\n",
      "I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
      "\n",
      "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\n",
      "\n",
      "There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming languages then were pretty primitive, and programmers' ideas correspondingly so. The default language at Cornell was a Pascal-like language called PL/I, and the situation was similar elsewhere. Learning Lisp expanded my concept of a program so fast that it was years before I started to have a sense of where the new limits were. This was more like it; this was what I had expected college to do. \n"
     ]
    }
   ],
   "source": [
    "print(nodes[1].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def read_text_file(filepath: str) -> str:\n",
    "    \"\"\"Read and return the contents of a text file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740204023.486078 1296402 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740204054.300441 1296402 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-huggingface fastembed -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/sentence-transformers/all-mpnet-base-v2/78c0197b6159d92658e319bc1d72e4c73a9a03dd03815e70e555c5ef05615658?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1739877539&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTg3NzUzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zZW50ZW5jZS10cmFuc2Zvcm1lcnMvYWxsLW1wbmV0LWJhc2UtdjIvNzhjMDE5N2I2MTU5ZDkyNjU4ZTMxOWJjMWQ3MmU0YzczYTlhMDNkZDAzODE1ZTcwZTU1NWM1ZWYwNTYxNTY1OD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=LxrLuuaVT-MiNCdceM0Irln8w%7EZtd241NpC22dRlnhWlXbX747ziRG9YbFTRbeC%7Evlrql8ZhPMDQIEFZNm5xandP51vXXueaKQ--HC26jQjTH-ijN9khQoFuOqnordTsHcB0xx8EuSkCmcQm-pxFhs%7EcZYzCYQ03XKVF0C6xJbERDF2j18Dq9VL7QMqE%7EuO3swnW7JG%7E5D%7EKwUsQ5yGl3sPdxVVdlJojU2GlSGPwo0rOxBro2Gao4bV5t0iQmveIHJsqagyJAgcFNGNxBnIMmzy3LFKjxM1LlFA-iR8A8A3yakHiIjDFhtnP4UtKwHZW5tMWkowgLSALoYeCE629xQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:754\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 754\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:878\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m    879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:759\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BaseSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/requests/models.py:826\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach sentence is converted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-mpnet-base-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:308\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    299\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[1;32m    302\u001b[0m     model_name_or_path,\n\u001b[1;32m    303\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    307\u001b[0m ):\n\u001b[0;32m--> 308\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    321\u001b[0m         model_name_or_path,\n\u001b[1;32m    322\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    330\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1739\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1741\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:81\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     84\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:181\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_only_kwargs)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/transformers/modeling_utils.py:3776\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3761\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3774\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3775\u001b[0m     }\n\u001b[0;32m-> 3776\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3778\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3779\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3781\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:469\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    467\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    468\u001b[0m         reset_sessions()  \u001b[38;5;66;03m# In case of SSLError it's best to reset the shared requests.Session objects\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_resume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_nb_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_nb_retries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_tqdm_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m         consistency_error_message\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    483\u001b[0m             actual_size\u001b[38;5;241m=\u001b[39mtemp_file\u001b[38;5;241m.\u001b[39mtell(),\n\u001b[1;32m    484\u001b[0m         )\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    454\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from fastembed import TextEmbedding\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings= FastEmbedEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\"),\n",
    "                                breakpoint_threshold_type= \"percentile\",\n",
    "                                breakpoint_threshold_amount=90\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of chunks : 29\n",
      "Chunk #0\n",
      "**Setting Up a Mobile Device for Company Email**\n",
      "\n",
      "**Prerequisites:**\n",
      "\n",
      "* Mobile device with a supported operating system (iOS, Android, or Windows)\n",
      "* Company email account credentials\n",
      "* Mobile device management (MDM) profile installed (if required by company policy)\n",
      "\n",
      "**Step 1: Ensure Mobile Device Management (MDM) Profile is Installed (if required)**\n",
      "\n",
      "If your company requires MDM for mobile devices, ensure that the profile is installed on your device. This profile will allow your device to connect to the company network and access company email. If you are unsure whether MDM is required, contact your IT department for assistance. **Step 2: Set Up Email Account on Mobile Device**\n",
      "\n",
      "1.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #1\n",
      "Go to the Settings app on your mobile device. 2. Select \"Mail\" or \"Email\" (depending on your device's operating system). 3. Tap \"Add Account\" or \"Create a new account\". 4. Select \"Exchange\" or \"Corporate\" as the account type. 5. Enter your company email address and password. 6. If prompted, enter the company's email server address (e.g., mail.company.com). 7. Select the desired synchronization options (e.g., sync email, contacts, calendar). **Step 3: Configure Email Settings**\n",
      "\n",
      "1. In the email account settings, select the \"Advanced\" or \"Security\" option. 2. Ensure that the \"Use SSL/TLS\" or \"Use secure connection\" option is enabled. 3. Set the authentication method to \"Username and Password\" or \"Domain\\Username\". 4. If prompted, enter your company's email domain (e.g., company.com). **Step 4: Verify Email Account**\n",
      "\n",
      "1. Wait for the email account to synchronize with the company email server. 2. Send a test email to yourself or a colleague to verify that email is sending and receiving correctly. **Troubleshooting Tips:**\n",
      "\n",
      "* If you encounter issues setting up your email account, ensure that your device has a stable internet connection and that your email credentials are correct. * If you are unable to connect to the company email server, contact your IT department for assistance. * If you experience issues with email synchronization, try restarting your device or checking the email account settings. **Additional Information:**\n",
      "\n",
      "* For security reasons, it is recommended to set up a password or PIN lock on your mobile device. * Company email policies may require additional security measures, such as encryption or two-factor authentication. Contact your IT department for more information.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #2\n",
      "* If you need further assistance or have questions about company email policies, contact your IT department or refer to the company's email policy documentation. **Resetting a Forgotten PIN**\n",
      "\n",
      "If you have forgotten your PIN, you can reset it using the following steps:\n",
      "\n",
      "**Step 1: Access the PIN Reset Tool**\n",
      "\n",
      "1. Go to the company's intranet homepage and click on the \"IT Support\" link at the top right corner of the page. 2. Click on the \"Self-Service\" tab and then select \"PIN Reset\" from the drop-down menu. 3. You will be redirected to the PIN Reset Tool login page. **Step 2: Authenticate with Your Credentials**\n",
      "\n",
      "1.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #3\n",
      "Enter your company username and password in the required fields. 2.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #4\n",
      "Click the \"Login\" button to authenticate. **Step 3: Verify Your Identity**\n",
      "\n",
      "1. You will be prompted to answer your security question. Enter your answer in the required field.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #5\n",
      "2.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #6\n",
      "Click the \"Next\" button to proceed. **Step 4: Reset Your PIN**\n",
      "\n",
      "1. Enter a new PIN in the required field. The PIN must be at least 8 characters long and contain a mix of uppercase and lowercase letters, numbers, and special characters. 2.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #7\n",
      "Re-enter your new PIN in the confirmation field to ensure accuracy. 3. Click the \"Reset PIN\" button to complete the process. **Step 5: Confirm PIN Reset**\n",
      "\n",
      "1. You will receive a confirmation message indicating that your PIN has been successfully reset. 2. Click the \"OK\" button to close the message. **Important Notes:**\n",
      "\n",
      "* If you are unable to reset your PIN using the self-service tool, please contact the IT Helpdesk for assistance. * It is recommended to keep your PIN confidential and not share it with anyone. * You can reset your PIN a maximum of 3 times in a 24-hour period. If you exceed this limit, you will need to contact the IT Helpdesk to reset your PIN. By following these steps, you should be able to reset your forgotten PIN and regain access to company systems and applications. If you encounter any issues during the process, please do not hesitate to reach out to the IT Helpdesk for further assistance. **Configuring VPN Access for Remote Workers**\n",
      "\n",
      "**Overview**\n",
      "\n",
      "This article provides step-by-step instructions for configuring VPN access for remote workers. This allows employees working from home or other remote locations to securely connect to the company network and access company resources. **Prerequisites**\n",
      "\n",
      "* The remote worker's device (laptop or desktop) must meet the company's minimum system requirements for VPN connectivity. * The remote worker must have a valid company login credentials. * The remote worker must have a stable internet connection. **Step 1: Install the VPN Client**\n",
      "\n",
      "1. Go to the company's software portal and download the VPN client software. 2.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #8\n",
      "Run the installer and follow the prompts to install the software. 3. Once installed, launch the VPN client software. **Step 2: Configure the VPN Connection**\n",
      "\n",
      "1. In the VPN client software, click on \"New Connection\" and select \"Company VPN\" as the connection type. 2. Enter the company's VPN server address: `vpn.company.com`. 3.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #9\n",
      "Enter your company login credentials (username and password). 4.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #10\n",
      "Select the \"Save\" button to save the connection settings. **Step 3: Establish the VPN Connection**\n",
      "\n",
      "1. Click on the \"Connect\" button to establish the VPN connection. 2.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #11\n",
      "You may be prompted to authenticate with two-factor authentication (2FA). Follow the prompts to complete 2FA.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #12\n",
      "3. Once connected, you will see a confirmation message indicating that you are connected to the company VPN. **Step 4: Verify VPN Connection**\n",
      "\n",
      "1. Open a web browser and navigate to `company.com`.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #13\n",
      "2. You should be able to access company resources, such as email, intranet, and file shares. 3. Verify that your device's IP address has changed to a company-assigned IP address. **Troubleshooting Tips**\n",
      "\n",
      "* If you encounter issues connecting to the VPN, ensure that your device meets the minimum system requirements and that your internet connection is stable. * If you are unable to access company resources, verify that you are connected to the VPN and that your device's IP address has changed to a company-assigned IP address. * If you continue to experience issues, contact the IT helpdesk for further assistance. **Security Reminders**\n",
      "\n",
      "* Always keep your VPN client software up-to-date. * Never share your login credentials with anyone. * Avoid using public Wi-Fi or unsecured networks to access company resources via VPN. By following these steps, remote workers can securely connect to the company network and access company resources from anywhere.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #14\n",
      "If you have any questions or issues, please contact the IT helpdesk for further assistance. **Troubleshooting Issues with Microsoft Office**\n",
      "\n",
      "This article provides steps to troubleshoot common issues with Microsoft Office applications, including Word, Excel, PowerPoint, and Outlook. **Step 1: Restart Microsoft Office**\n",
      "\n",
      "Before proceeding with troubleshooting, try restarting the Microsoft Office application that is experiencing issues. This simple step can often resolve problems caused by temporary glitches or corrupted files. **Step 2: Check for Updates**\n",
      "\n",
      "Ensure that Microsoft Office is up-to-date. Open any Microsoft Office application, go to **File** > **Account**, and click **Update Options**. If updates are available, install them and restart the application. **Step 3: Disable Add-ins**\n",
      "\n",
      "Add-ins can sometimes cause issues with Microsoft Office applications. To disable add-ins, follow these steps:\n",
      "\n",
      "* Open the Microsoft Office application experiencing issues. * Go to **File** > **Options** > **Add-ins**.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #15\n",
      "* Uncheck the boxes next to each add-in to disable them. * Click **OK** to save changes. **Step 4: Check for Corrupt Files**\n",
      "\n",
      "Corrupt files can cause issues with Microsoft Office applications. Try opening a new, blank document to see if the issue persists. If the issue does not occur with a new document, it's possible that the original file is corrupt. **Step 5: Check for Conflicting Programs**\n",
      "\n",
      "Other programs running on your computer can conflict with Microsoft Office applications.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #16\n",
      "Try closing all other programs and background applications to see if the issue resolves. **Step 6: Check Office Configuration**\n",
      "\n",
      "Office configuration issues can cause problems with Microsoft Office applications. Try resetting Office configuration by following these steps:\n",
      "\n",
      "* Open the **Command Prompt** as an administrator. * Type the following command and press **Enter**: `officec2rclient.exe /uninstall`\n",
      "* Wait for the command to complete, then restart your computer. **Step 7: Reinstall Microsoft Office**\n",
      "\n",
      "If none of the above steps resolve the issue, try reinstalling Microsoft Office. Go to the **Control Panel**, select **Programs and Features**, and uninstall Microsoft Office. Then, reinstall Microsoft Office from the company's software portal or by contacting the IT department. **Additional Troubleshooting Tips**\n",
      "\n",
      "* Check for antivirus software conflicts, as some antivirus programs can interfere with Microsoft Office applications. * Ensure that your computer meets the system requirements for Microsoft Office. * If using a Microsoft Office template, try creating a new document from scratch to see if the issue persists. * If experiencing issues with a specific file, try opening the file on a different computer or in a different Microsoft Office application to isolate the problem. By following these steps, you should be able to troubleshoot and resolve common issues with Microsoft Office applications.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #17\n",
      "If the issue persists, contact the IT department for further assistance. To set up a conference call on Cisco Webex, follow these steps:\n",
      "\n",
      "**Step 1: Log in to Cisco Webex**\n",
      "\n",
      "Open a web browser and navigate to [company Webex URL]. Enter your company login credentials to access the Webex portal. **Step 2: Schedule a Meeting**\n",
      "\n",
      "Click on the \"Meetings\" tab and select \"Schedule a Meeting\" from the drop-down menu. Fill in the required information, including:\n",
      "\n",
      "* Meeting topic\n",
      "* Start and end time\n",
      "* Duration\n",
      "* Invitees (add email addresses or names from the company directory)\n",
      "\n",
      "**Step 3: Configure Audio Settings**\n",
      "\n",
      "In the \"Audio\" section, select \"Cisco Webex Audio\" as the default audio type. You can also choose to allow participants to join via phone or VoIP. **Step 4: Set Up Video Settings (Optional)**\n",
      "\n",
      "If you want to enable video conferencing, click on the \"Video\" tab and select \"Cisco Webex Video\" as the default video type. You can also choose to allow participants to join via video. **Step 5: Add Agenda and Notes (Optional)**\n",
      "\n",
      "You can add an agenda and notes to the meeting invitation by clicking on the \"Agenda\" tab. This information will be visible to all invitees. **Step 6: Send Invitations**\n",
      "\n",
      "Click \"Schedule\" to send the meeting invitation to all invitees. The invitation will include a link to join the meeting, as well as dial-in information for audio-only participants. **Step 7: Start the Meeting**\n",
      "\n",
      "At the scheduled start time, click on the \"Start\" button to begin the conference call. As the host, you will be prompted to enter your audio and video settings before joining the meeting. **Tips and Best Practices**\n",
      "\n",
      "* Make sure to test your audio and video settings before the meeting to ensure a smooth experience. * Encourage invitees to join the meeting 5-10 minutes early to troubleshoot any technical issues. * Use the \"Mute\" feature to minimize background noise and ensure clear audio. * Use the \"Share\" feature to share your screen or presentations with participants. **Troubleshooting**\n",
      "\n",
      "* If you encounter issues joining the meeting, try restarting your browser or checking your internet connection.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #18\n",
      "* If you experience audio or video issues during the meeting, try restarting your audio or video settings or contacting the IT helpdesk for assistance. By following these steps, you can successfully set up and host a conference call on Cisco Webex. If you have any further questions or issues, please contact the IT helpdesk for assistance.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #19\n",
      "**Creating a Backup of Important Files**\n",
      "\n",
      "Backing up important files is a crucial step in protecting your data from loss or corruption. This article will guide you through the process of creating a backup of your important files. **Step 1: Identify Important Files**\n",
      "\n",
      "The first step in creating a backup is to identify the important files that need to be backed up. These may include documents, spreadsheets, presentations, and other files that are critical to your work. Make a list of the files and folders that you need to back up. **Step 2: Choose a Backup Method**\n",
      "\n",
      "There are several ways to back up your files, including:\n",
      "\n",
      "* **External Hard Drive**: Connect an external hard drive to your computer and copy your important files to it. * **Cloud Backup**: Use a cloud backup service such as OneDrive, Google Drive, or Dropbox to store your files online. * **Network Share**: Save your files to a network share or a shared drive on your company's network. * **USB Drive**: Use a USB drive to store your important files. Choose a backup method that works best for you and your needs. **Step 3: Connect Your Backup Device**\n",
      "\n",
      "If you are using an external hard drive or USB drive, connect it to your computer. If you are using a cloud backup service, log in to your account and ensure that you have sufficient storage space. **Step 4: Copy Your Files**\n",
      "\n",
      "Copy your important files to your chosen backup device. Make sure to copy all the files and folders that you identified in Step 1. **Step 5: Verify Your Backup**\n",
      "\n",
      "Once you have copied your files, verify that they have been backed up successfully. Check the contents of your backup device to ensure that all files are present and can be opened. **Step 6: Schedule Regular Backups**\n",
      "\n",
      "To ensure that your important files are always up to date, schedule regular backups. You can set up your backup software to run automatically at a specified time each day or week. **Additional Tips**\n",
      "\n",
      "* **Backup Frequency**: Aim to backup your important files at least once a week, or more frequently if you work with sensitive or critical data. * **Backup Location**: Store your backup device in a safe location, away from your computer and other electronic devices. * **Versioning**: Consider using a backup software that supports versioning, which allows you to keep multiple versions of your files in case you need to recover a previous version. By following these steps, you can ensure that your important files are safely backed up and protected from loss or corruption. **Troubleshooting Issues with Company-Issued Tablets**\n",
      "\n",
      "This article provides step-by-step guidance for troubleshooting common issues with company-issued tablets. Please follow the steps outlined below to resolve the issue before escalating to advanced support. **Step 1: Power Cycle the Tablet**\n",
      "\n",
      "* Press and hold the power button until the tablet shuts down. * Wait for 30 seconds to allow any residual power to drain. * Press the power button again to turn the tablet back on. * Observe if the issue persists. **Step 2: Check for Software Updates**\n",
      "\n",
      "* Ensure the tablet is connected to a stable internet connection. * Go to **Settings** > **System** > **System Update**. * Check for available software updates and install any pending updates.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #20\n",
      "* Restart the tablet after installation. **Step 3: Restart in Safe Mode**\n",
      "\n",
      "* Press and hold the power button until the tablet shuts down. * Press the power button again to turn the tablet back on. * Immediately hold down the **Volume Down** button until the tablet boots into safe mode. * Observe if the issue persists in safe mode. **Step 4: Check for App-Related Issues**\n",
      "\n",
      "* Identify the app that is experiencing issues. * Go to **Settings** > **Apps** > **[App Name]**. * Clear the app's cache and data. * Restart the app to see if the issue is resolved. **Step 5: Perform a Factory Reset**\n",
      "\n",
      "* Go to **Settings** > **System** > **Advanced** > **Reset Options**. * Select **Erase all data (factory reset)**. * Follow the prompts to complete the reset process. * Note: This step will erase all data on the tablet, so ensure that all important data is backed up before proceeding. **Step 6: Check for Hardware Issues**\n",
      "\n",
      "* Inspect the tablet for any signs of physical damage or wear. * Check the charging port and ensure it is free from debris. * Try using a different charging cable or power source. **Step 7: Gather Diagnostic Information**\n",
      "\n",
      "* Go to **Settings** > **System** > **About Tablet**. * Take note of the tablet's operating system version, device ID, and any error messages. * Provide this information to advanced support if the issue cannot be resolved through these troubleshooting steps.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #21\n",
      "If none of the above steps resolve the issue, please escalate the issue to advanced support, providing as much detail as possible about the problem and the steps taken so far. **Step 1: Plan Your Wireless Network**\n",
      "\n",
      "Before setting up a secure wireless network, plan your network infrastructure by identifying the number of users, devices, and access points required. Determine the type of encryption and authentication methods to be used.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #22\n",
      "Consider the physical layout of your workspace and the range of your wireless signal. **Step 2: Choose a Secure Wireless Protocol**\n",
      "\n",
      "Select a secure wireless protocol such as WPA2-PSK (AES) or WPA3-PSK (AES-256). Avoid using outdated protocols like WEP or WPA. Ensure that your devices and access points support the chosen protocol. **Step 3: Set Up a Strong Password**\n",
      "\n",
      "Create a strong password for your wireless network. The password should be at least 12 characters long and include a mix of uppercase and lowercase letters, numbers, and special characters. Avoid using easily guessable passwords or common words. **Step 4: Configure Your Router**\n",
      "\n",
      "Log in to your router's web interface using a secure connection (HTTPS). Change the default administrator password and network name (SSID). Enable WPA2-PSK or WPA3-PSK encryption and set the password created in Step 3. Save your changes. **Step 5: Set Up a Guest Network (Optional)**\n",
      "\n",
      "If you need to provide wireless access to guests or visitors, set up a separate guest network. This will isolate your main network from potential security risks. Configure the guest network with a different SSID and password. **Step 6: Configure Firewall Settings**\n",
      "\n",
      "Enable the firewall on your router and configure it to block incoming and outgoing traffic on unnecessary ports. Restrict access to your network by only allowing necessary incoming traffic. **Step 7: Set Up Quality of Service (QoS)**\n",
      "\n",
      "Configure QoS settings on your router to prioritize critical network traffic, such as video conferencing or VoIP calls. This ensures that critical applications receive sufficient bandwidth. **Step 8: Implement Network Segmentation**\n",
      "\n",
      "Segment your network into different zones or subnets to restrict access to sensitive areas of your network. This limits the spread of malware or unauthorized access in case of a breach. **Step 9: Regularly Update Your Router's Firmware**\n",
      "\n",
      "Regularly check for and install firmware updates for your router to ensure you have the latest security patches and features. **Step 10: Monitor Your Network**\n",
      "\n",
      "Regularly monitor your network for suspicious activity, such as unknown devices or unusual traffic patterns. Use network monitoring tools or work with your IT team to detect and respond to potential security incidents. By following these steps, you can set up a secure wireless network that protects your data and devices from unauthorized access.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #23\n",
      "Remember to regularly review and update your network configuration to ensure ongoing security. **Resetting a Jammed Printer**\n",
      "\n",
      "**Step 1: Turn Off the Printer**\n",
      "\n",
      "Immediately turn off the printer to prevent any further damage or paper jams. Locate the power button, usually found on the top or front of the printer, and press it to shut down the device. **Step 2: Open the Printer**\n",
      "\n",
      "Locate the printer's access panel, which varies depending on the printer model. Common locations include the top, front, or back of the printer. Open the panel by pressing the release latch or sliding it open. This will give you access to the internal mechanisms. **Step 3: Identify and Remove Jammed Paper**\n",
      "\n",
      "Carefully inspect the printer's internal mechanisms to locate the jammed paper. Gently pull out any visible paper scraps or torn pieces. Be cautious not to touch any internal components or rollers, as oils from your skin can cause damage. **Step 4: Check for Obstructions**\n",
      "\n",
      "Inspect the printer's paper path, including the paper tray, rollers, and print head, for any obstructions or debris. Remove any dust, dirt, or other blockages that may be contributing to the jam. **Step 5: Realign the Paper Tray**\n",
      "\n",
      "If the paper tray was dislodged during the jam, realign it according to the manufacturer's instructions. Make sure it clicks securely into place.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #24\n",
      "**Step 6: Check and Clean the Print Head**\n",
      "\n",
      "Check the print head for any blockages or dried ink. Gently clean the print head with a lint-free cloth or cotton swab, following the manufacturer's instructions. **Step 7: Reassemble the Printer**\n",
      "\n",
      "Reassemble the printer in the reverse order of how you disassembled it. Make sure all panels and access doors are securely closed. **Step 8: Power On and Test**\n",
      "\n",
      "Turn the printer back on and test it by printing a test page or a document. If the printer is still experiencing issues, refer to the manufacturer's troubleshooting guide or contact the IT helpdesk for further assistance. **Additional Tips:**\n",
      "\n",
      "* Regularly clean and maintain your printer to prevent paper jams and other issues. * Use high-quality paper that is designed for printing to reduce the likelihood of jams. * Avoid overloading the paper tray, as this can cause paper jams and other issues. * If you are unsure about how to reset a jammed printer or if the issue persists after following these steps, contact the IT helpdesk for assistance. **Configuring Email on an Android Device**\n",
      "\n",
      "This article provides step-by-step instructions on how to configure email on an Android device. Please follow the steps below to set up your email account on your Android device. **Prerequisites:**\n",
      "\n",
      "* An Android device with a valid internet connection\n",
      "* A valid email account (company email or personal email)\n",
      "* Email account credentials (username and password)\n",
      "\n",
      "**Step 1: Go to the Email App**\n",
      "\n",
      "* Locate the Email app on your Android device. It may be on the home screen or in the app drawer.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #25\n",
      "* Tap on the Email app to open it. **Step 2: Add a New Account**\n",
      "\n",
      "* Tap on the \"Add account\" or \"Add email account\" button. * Select \"Email\" or \"Corporate\" as the account type, depending on your email provider. * Enter your email address and password in the required fields.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #26\n",
      "* Tap \"Next\" to proceed. **Step 3: Configure Server Settings**\n",
      "\n",
      "* The device will attempt to automatically configure the server settings. If it fails, you will need to enter the settings manually.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #27\n",
      "* Tap on \"Manual setup\" or \"Advanced setup\" to enter the server settings manually. * Enter the following information:\n",
      "\t+ Incoming server: [company email server or personal email server]\n",
      "\t+ Port: [company email port or personal email port]\n",
      "\t+ Security type: [company email security type or personal email security type]\n",
      "\t+ Username: [your email address]\n",
      "\t+ Password: [your email password]\n",
      "* Tap \"Next\" to proceed. **Step 4: Configure Outgoing Server Settings**\n",
      "\n",
      "* Enter the following information:\n",
      "\t+ Outgoing server: [company email server or personal email server]\n",
      "\t+ Port: [company email port or personal email port]\n",
      "\t+ Security type: [company email security type or personal email security type]\n",
      "\t+ Username: [your email address]\n",
      "\t+ Password: [your email password]\n",
      "* Tap \"Next\" to proceed. **Step 5: Account Options**\n",
      "\n",
      "* You may be prompted to configure additional account options, such as:\n",
      "\t+ Sync frequency: Choose how often you want your device to sync with the email server. + Sync size: Choose the maximum size of emails to sync. + Delete email from server: Choose whether to delete emails from the server after syncing. * Tap \"Next\" to proceed.\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "Chunk #28\n",
      "**Step 6: Account Setup Complete**\n",
      "\n",
      "* You will be prompted to give your account a name and set it as the default account. * Tap \"Done\" to complete the setup. **Troubleshooting Tips:**\n",
      "\n",
      "* If you encounter any issues during the setup process, try restarting the Email app or your device. * Ensure that your email account credentials are correct and that your internet connection is stable. * If you are still experiencing issues, contact the IT helpdesk for further assistance. By following these steps, you should be able to successfully configure email on your Android device.\n",
      "...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = 'test.txt'\n",
    "essay = read_text_file(filepath)\n",
    "chunks = text_splitter.split_text(essay)\n",
    "print(f\"length of chunks : {len(chunks)}\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk #{i}\")\n",
    "        print(chunk.strip())\n",
    "        print(\"...\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: <urlopen error [Errno 8] nodename nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "url = \"http://corpus.irisa.fr/texmex/corpus/sift.tar.gz\"  # Try HTTP if available\n",
    "\n",
    "try:\n",
    "    with urllib.request.urlopen(url) as response, open(\"sift.tar.gz\", \"wb\") as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    print(\"Download successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading the dataset: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "import shutil\n",
    "\n",
    "# Create an unverified SSL context (if needed)\n",
    "ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "# URL for the ANN_SIFT10K dataset\n",
    "url = \"https://ann-benchmarks.com/sift-10k.tar.gz\"\n",
    "output_filename = \"sift-10k.tar.gz\"\n",
    "\n",
    "# Create a Request with a custom User-Agent header\n",
    "req = urllib.request.Request(\n",
    "    url,\n",
    "    headers={'User-Agent': 'Mozilla/5.0'}\n",
    ")\n",
    "\n",
    "try:\n",
    "    with urllib.request.urlopen(req, context=ssl_context) as response, open(output_filename, \"wb\") as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error downloading the dataset:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                         sentence_A  \\\n",
       "0        1  A group of kids is playing in a yard and an ol...   \n",
       "1        2  A group of children is playing in the house an...   \n",
       "2        3  The young boys are playing outdoors and the ma...   \n",
       "3        5  The kids are playing outdoors near a man with ...   \n",
       "4        9  The young boys are playing outdoors and the ma...   \n",
       "\n",
       "                                          sentence_B  relatedness_score  \\\n",
       "0  A group of boys in a yard is playing and a man...                4.5   \n",
       "1  A group of kids is playing in a yard and an ol...                3.2   \n",
       "2  The kids are playing outdoors near a man with ...                4.7   \n",
       "3  A group of kids is playing in a yard and an ol...                3.4   \n",
       "4  A group of kids is playing in a yard and an ol...                3.7   \n",
       "\n",
       "  entailment_judgment  \n",
       "0             NEUTRAL  \n",
       "1             NEUTRAL  \n",
       "2          ENTAILMENT  \n",
       "3             NEUTRAL  \n",
       "4             NEUTRAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "res = requests.get('https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt')\n",
    "# create dataframe\n",
    "data = pd.read_csv(StringIO(res.text), sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we take all samples from both sentence A and B\n",
    "sentences = data['sentence_A'].tolist()\n",
    "sentence_b = data['sentence_B'].tolist()\n",
    "sentences.extend(sentence_b)  # merge them\n",
    "len(set(sentences))  # together we have ~4.5K unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and NaN\n",
    "sentences = [word for word in list(set(sentences)) if type(word) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# initialize sentence transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# create sentence embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4802, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "d = sentence_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.is_trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.add(sentence_embeddings)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "xq = model.encode([\"A group of boys in a yard is playing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 ms, sys: 2.3 ms, total: 4.63 ms\n",
      "Wall time: 5.51 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "D, I = index.search(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3981 3641 3020 4074]]\n"
     ]
    }
   ],
   "source": [
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3981    A brown and white dog with a brown and black b...\n",
       "3641         A white dog is running through a snowy trail\n",
       "3020    A classroom full of students is looking in the...\n",
       "4074    A pair of kids are sticking out blue and green...\n",
       "Name: sentence_A, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentence_A'].iloc[[3981 ,3641, 3020, 4074]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 50  # how many cells\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "index.train(sentence_embeddings)\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piencone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"pinecone[grpc]\" -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama-text-embed-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Piencone\n",
    "from pinecone import ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_345tD4_4PC4nTwKFbe2Vivjf3W9D9ZMGH9GjkqLu5XzKk1K2c8t9p2ZhDbeurFoj6ERgXw\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Piencone(api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "              ssl_verify= False\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"id\": \"rec1\",\n",
    "        \"text\": \"Apples are a great source of dietary fiber, which supports digestion and helps maintain a healthy gut.\",\n",
    "        \"category\": \"digestive system\" \n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rec2\",\n",
    "        \"text\": \"Apples originated in Central Asia and have been cultivated for thousands of years, with over 7,500 varieties available today.\",\n",
    "        \"category\": \"cultivation\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rec3\",\n",
    "        \"text\": \"Rich in vitamin C and other antioxidants, apples contribute to immune health and may reduce the risk of chronic diseases.\",\n",
    "        \"category\": \"immune system\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rec4\",\n",
    "        \"text\": \"The high fiber content in apples can also help regulate blood sugar levels, making them a favorable snack for people with diabetes.\",\n",
    "        \"category\": \"endocrine system\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.pinecone.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = pc.inference.embed(\n",
    "    model= \"multilingual-e5-large\",\n",
    "    inputs= [ x[\"text\"] for x in data],\n",
    "    parameters={\n",
    "        \"input_type\" : \"passage\",\n",
    "        \"truncate\" :  \"END\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingsList(\n",
       "  model='multilingual-e5-large',\n",
       "  vector_type='dense',\n",
       "  data=[\n",
       "    {'vector_type': dense, 'values': [0.04205322265625, -0.00951385498046875, ..., -0.050506591796875, -0.01019287109375]},\n",
       "    {'vector_type': dense, 'values': [0.033203125, -0.018524169921875, ..., -0.00965118408203125, -0.0240631103515625]},\n",
       "    {'vector_type': dense, 'values': [0.033599853515625, -0.00600433349609375, ..., -0.005611419677734375, -0.0251922607421875]},\n",
       "    {'vector_type': dense, 'values': [0.00972747802734375, -0.01181793212890625, ..., -0.0252227783203125, -0.006389617919921875]}\n",
       "  ],\n",
       "  usage={'total_tokens': 118}\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.pinecone.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "index_name = \"pinecone-demo\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name= index_name,\n",
    "        dimension= 128,\n",
    "        metric= \"cosine\",\n",
    "        spec= ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ),\n",
    "        \n",
    "    )\n",
    "    \n",
    "while not pc.describe_index(name=index_name).status[\"ready\"]:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upserted_count: 4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(name= index_name)\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "\n",
    "for d,e in zip(data, embeddings):\n",
    "    records.append({\n",
    "        \"id\" : d[\"id\"],\n",
    "        \"values\" : e[\"values\"],\n",
    "        \"metadata\": {\n",
    "            \"source_text\" : d[\"text\"],\n",
    "            \"category\" : d[\"category\"]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    \n",
    "    \n",
    "index.upsert(vectors= records,\n",
    "             namespace= \"version-0\",\n",
    "             show_progress= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'version-0': {'vector_count': 4}},\n",
      " 'total_vector_count': 4}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Health risks\"\n",
    "\n",
    "query_embedding = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs= [query],\n",
    "    parameters= {\n",
    "        \"input_type\" : \"query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "results = index.query(\n",
    "    vector=query_embedding[0].values,\n",
    "    namespace=\"version-0\",\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'rec3',\n",
       "              'metadata': {'category': 'immune system',\n",
       "                           'source_text': 'Rich in vitamin C and other '\n",
       "                                          'antioxidants, apples contribute to '\n",
       "                                          'immune health and may reduce the '\n",
       "                                          'risk of chronic diseases.'},\n",
       "              'score': 0.8223334,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': 'rec1',\n",
       "              'metadata': {'category': 'digestive system',\n",
       "                           'source_text': 'Apples are a great source of '\n",
       "                                          'dietary fiber, which supports '\n",
       "                                          'digestion and helps maintain a '\n",
       "                                          'healthy gut.'},\n",
       "              'score': 0.79157746,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': 'rec4',\n",
       "              'metadata': {'category': 'endocrine system',\n",
       "                           'source_text': 'The high fiber content in apples '\n",
       "                                          'can also help regulate blood sugar '\n",
       "                                          'levels, making them a favorable '\n",
       "                                          'snack for people with diabetes.'},\n",
       "              'score': 0.7857753,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []}],\n",
       " 'namespace': 'version-0',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RerankResult(\n",
       "  model='bge-reranker-v2-m3',\n",
       "  data=[{\n",
       "    index=0,\n",
       "    score=0.07992552,\n",
       "    document={\n",
       "        id='rec3',\n",
       "        source_text='Rich in vitamin C and other antioxidants, apples contribute to immune health and may reduce the risk of chronic diseases.'\n",
       "    }\n",
       "  },{\n",
       "    index=2,\n",
       "    score=0.003976228,\n",
       "    document={\n",
       "        id='rec4',\n",
       "        source_text='The high fiber content in apples can also help regulate blood sugar levels, making them a favorable snack for people with diabetes.'\n",
       "    }\n",
       "  },{\n",
       "    index=1,\n",
       "    score=0.0009888597,\n",
       "    document={\n",
       "        id='rec1',\n",
       "        source_text='Apples are a great source of dietary fiber, which supports digestion and helps maintain a healthy gut.'\n",
       "    }\n",
       "  }],\n",
       "  usage={'rerank_units': 1}\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_result = pc.inference.rerank(\n",
    "    model=\"bge-reranker-v2-m3\",\n",
    "    query= query,\n",
    "    documents=[\n",
    "        {\"id\": \"rec3\", \"source_text\": \"Rich in vitamin C and other antioxidants, apples contribute to immune health and may reduce the risk of chronic diseases.\"},\n",
    "        {\"id\": \"rec1\", \"source_text\": \"Apples are a great source of dietary fiber, which supports digestion and helps maintain a healthy gut.\"},\n",
    "        {\"id\": \"rec4\", \"source_text\": \"The high fiber content in apples can also help regulate blood sugar levels, making them a favorable snack for people with diabetes.\"}\n",
    "    ],\n",
    "    top_n=3,\n",
    "    return_documents= True,\n",
    "    parameters= {\n",
    "        \"truncate\" : \"END\"\n",
    "    },\n",
    "    rank_fields=[\"source_text\"]\n",
    "    \n",
    ")\n",
    "\n",
    "ranked_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'rec1',\n",
       "              'metadata': {'category': 'digestive system',\n",
       "                           'source_text': 'Apples are a great source of '\n",
       "                                          'dietary fiber, which supports '\n",
       "                                          'digestion and helps maintain a '\n",
       "                                          'healthy gut.'},\n",
       "              'score': 0.7912986,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []}],\n",
       " 'namespace': 'version-0',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterd_result = index.query(\n",
    "    vector= query_embedding[0].values,\n",
    "    namespace= \"version-0\",\n",
    "    top_k=3,\n",
    "    filter={\n",
    "        \"category\" : {\"$eq\" : \"digestive system\"}\n",
    "        \n",
    "    },\n",
    "            include_metadata=True,\n",
    "        include_values= False,\n",
    ")\n",
    "\n",
    "filterd_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.configure_index(\n",
    "    name=index_name,\n",
    "    deletion_protection=\"disabled\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def chunks(iterable, batch_size=200):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "vector_dim = 128\n",
    "vector_count = 1000\n",
    "\n",
    "# Example generator that generates many (id, vector) pairs\n",
    "example_data_generator = map(lambda i: (f'id-{i}', [random.random() for _ in range(vector_dim)]), range(vector_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x123943100>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids_vectors_chunk in chunks(example_data_generator, batch_size=200):\n",
    "    index.upsert(vectors=ids_vectors_chunk) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.grpc.index_grpc.GRPCIndex at 0x117e1f160>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"v0-sparse-index\",\n",
       "    \"metric\": \"dotproduct\",\n",
       "    \"host\": \"v0-sparse-index-auoio4m.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"sparse\",\n",
       "    \"dimension\": null,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.create_index(\n",
    "    name= \"v0-sparse-index\",\n",
    "    spec= ServerlessSpec(cloud=\"aws\", region= \"us-east-1\"),\n",
    "    metric=\"dotproduct\",\n",
    "    vector_type=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upserted_count: 4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pinecone import Pinecone, SparseValues, Vector\n",
    "index = pc.Index(\"v0-sparse-index\")\n",
    "\n",
    "index.upsert(\n",
    "    namespace=\"user0\",\n",
    "    vectors=[\n",
    "        Vector(\n",
    "            id=\"vec1\",\n",
    "            sparse_values=SparseValues(\n",
    "                values=[1.7958984, 0.41577148, 2.828125, 2.8027344, 2.8691406, 1.6533203, 5.3671875, 1.3046875, 0.49780273, 0.5722656, 2.71875, 3.0820312, 2.5019531, 4.4414062, 3.3554688],\n",
    "                indices=[822745112, 1009084850, 1221765879, 1408993854, 1504846510, 1596856843, 1640781426, 1656251611, 1807131503, 2543655733, 2902766088, 2909307736, 3246437992, 3517203014, 3590924191]\n",
    "            ),\n",
    "            metadata={\n",
    "                \"source_text\": \"AAPL reported a year-over-year revenue increase, expecting stronger Q3 demand for its flagship phones.\",\n",
    "                \"category\": \"technology\",\n",
    "                \"quarter\": \"Q3\"\n",
    "            }\n",
    "        ),\n",
    "        Vector(\n",
    "            id=\"vec2\",\n",
    "            sparse_values=SparseValues(\n",
    "                values=[0.4362793, 3.3457031, 2.7714844, 3.0273438, 3.3164062, 5.6015625, 2.4863281, 0.38134766, 1.25, 2.9609375, 0.34179688, 1.4306641, 0.34375, 3.3613281, 1.4404297, 2.2558594, 2.2597656, 4.8710938, 0.5605469],\n",
    "                indices=[131900689, 592326839, 710158994, 838729363, 1304885087, 1640781426, 1690623792, 1807131503, 2066971792, 2428553208, 2548600401, 2577534050, 3162218338, 3319279674, 3343062801, 3476647774, 3485013322, 3517203014, 4283091697]\n",
    "            ),\n",
    "            metadata={\n",
    "                \"source_text\": \"Analysts suggest that AAPL'\\''s upcoming Q4 product launch event might solidify its position in the premium smartphone market.\",\n",
    "                \"category\": \"technology\",\n",
    "                \"quarter\": \"Q4\"\n",
    "            }\n",
    "        ),\n",
    "        Vector(\n",
    "            id=\"vec3\",\n",
    "            sparse_values=SparseValues(\n",
    "                values=[2.6875, 4.2929688, 3.609375, 3.0722656, 2.1152344, 5.78125, 3.7460938, 3.7363281, 1.2695312, 3.4824219, 0.7207031, 0.0826416, 4.671875, 3.7011719, 2.796875, 0.61621094],\n",
    "                indices=[8661920, 350356213, 391213188, 554637446, 1024951234, 1640781426, 1780689102, 1799010313, 2194093370, 2632344667, 2641553256, 2779594451, 3517203014, 3543799498, 3837503950, 4283091697]\n",
    "            ),\n",
    "            metadata={\n",
    "                \"source_text\": \"AAPL'\\''s strategic Q3 partnerships with semiconductor suppliers could mitigate component risks and stabilize iPhone production\",\n",
    "                \"category\": \"technology\",\n",
    "                \"quarter\": \"Q3\"\n",
    "            }\n",
    "        ),\n",
    "        Vector(\n",
    "            id=\"vec4\",\n",
    "            sparse_values=SparseValues(\n",
    "                values=[0.73046875, 0.46972656, 2.84375, 5.2265625, 3.3242188, 1.9863281, 0.9511719, 0.5019531, 4.4257812, 3.4277344, 0.41308594, 4.3242188, 2.4179688, 3.1757812, 1.0224609, 2.0585938, 2.5859375],\n",
    "                indices=[131900689, 152217691, 441495248, 1640781426, 1851149807, 2263326288, 2502307765, 2641553256, 2684780967, 2966813704, 3162218338, 3283104238, 3488055477, 3530642888, 3888762515, 4152503047, 4177290673]\n",
    "            ),\n",
    "            metadata={\n",
    "                \"source_text\": \"AAPL may consider healthcare integrations in Q4 to compete with tech rivals entering the consumer wellness space.\",\n",
    "                \"category\": \"technology\",\n",
    "                \"quarter\": \"Q4\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = index.query(\n",
    "    namespace=\"user0\",\n",
    "    sparse_vector={\n",
    "      \"values\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "      \"indices\": [767227209, 1640781426, 1690623792, 2021799277, 2152645940, 2295025838, 2443437770, 2779594451, 2956155693, 3476647774, 3818127854, 4283091697]\n",
    "    },\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'vec2',\n",
       "              'metadata': {'category': 'technology',\n",
       "                           'quarter': 'Q4',\n",
       "                           'source_text': \"Analysts suggest that AAPL'''s \"\n",
       "                                          'upcoming Q4 product launch event '\n",
       "                                          'might solidify its position in the '\n",
       "                                          'premium smartphone market.'},\n",
       "              'score': 10.904297,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': 'vec3',\n",
       "              'metadata': {'category': 'technology',\n",
       "                           'quarter': 'Q3',\n",
       "                           'source_text': \"AAPL'''s strategic Q3 partnerships \"\n",
       "                                          'with semiconductor suppliers could '\n",
       "                                          'mitigate component risks and '\n",
       "                                          'stabilize iPhone production'},\n",
       "              'score': 6.4801025,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': 'vec1',\n",
       "              'metadata': {'category': 'technology',\n",
       "                           'quarter': 'Q3',\n",
       "                           'source_text': 'AAPL reported a year-over-year '\n",
       "                                          'revenue increase, expecting '\n",
       "                                          'stronger Q3 demand for its flagship '\n",
       "                                          'phones.'},\n",
       "              'score': 5.3671875,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []}],\n",
       " 'namespace': 'user0',\n",
       " 'usage': {'read_units': 1}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymilvus -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(\"milvus_demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.has_collection(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_collection(\"demo_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.has_collection(collection_name=\"demo_collection\"):\n",
    "    client.drop_collection(collection_name=\"demo_collection\")\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    dimension=768,  # The vectors we will use in this demo has 768 dimensions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 3 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'subject'])\n",
      "Vector dim: 768\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "docs = [\n",
    "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "]\n",
    "vectors = [[random.uniform(-1, 1) for _ in range(768)] for _ in docs]\n",
    "data = [\n",
    "    {\"id\": i, \"vector\": vectors[i], \"text\": docs[i], \"subject\": \"history\"}\n",
    "    for i in range(len(vectors))\n",
    "]\n",
    "\n",
    "print(\"Data has\", len(data), \"entities, each with fields: \", data[0].keys())\n",
    "print(\"Vector dim:\", len(data[0][\"vector\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.insert(collection_name= \"demo_collection\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'insert_count': 3, 'ids': [0, 1, 2], 'cost': 0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vectors = [ [ random.uniform(-1, 1) for _ in range(768) ] ]\n",
    "\n",
    "res = client.search(\n",
    "    collection_name= \"demo_collection\",\n",
    "    output_fields=[\"text\", \"subject\"],\n",
    "    data= query_vectors,\n",
    "    filter= \"subject == 'history'\",\n",
    "    limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [\"[{'id': 1, 'distance': 0.009793841280043125, 'entity': {'text': 'Alan Turing was the first person to conduct substantial research in AI.', 'subject': 'history'}}, {'id': 0, 'distance': -0.009418610483407974, 'entity': {'text': 'Artificial intelligence was founded as an academic discipline in 1956.', 'subject': 'history'}}]\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= client.query(\n",
    "    collection_name= \"demo_collection\",\n",
    "    ids=[0],\n",
    "    output_fields=[\"text\", \"subject\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [\"{'id': 0, 'text': 'Artificial intelligence was founded as an academic discipline in 1956.', 'subject': 'history'}\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete(collection_name=\"demo_collection\", ids = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_collection(collection_name=\"dmeo_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_id': True, 'description': '', 'fields': [{'name': 'my_id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'my_vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 5}}, {'name': 'my_varchar', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 512}}], 'enable_dynamic_field': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymilvus import DataType\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id = True,\n",
    "    enable_dynamic_field = True\n",
    ")\n",
    "\n",
    "schema.add_field(field_name=\"my_id\", datatype=DataType.INT64, is_primary =True)\n",
    "schema.add_field(field_name=\"my_vector\", datatype=DataType.FLOAT_VECTOR, dim=5)\n",
    "schema.add_field(field_name=\"my_varchar\", datatype=DataType.VARCHAR, max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"my_vector\", \n",
    "    index_type=\"AUTOINDEX\",\n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_collection(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': <LoadState: Loaded>}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"demo\",\n",
    "    schema= schema,\n",
    "    index_params=index_params,\n",
    "    mum_shard = 1,\n",
    "    enable_mnap = True,\n",
    "    consistancy_level = \"Bounded\"\n",
    ")\n",
    "\n",
    "client.get_load_state(collection_name=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.release_collection(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.load_collection(collection_name=\"demo\",\n",
    "                      load_fields = [\"my_vector\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_partition(\n",
    "    collection_name=\"demo\",\n",
    "    partition_name=\"p1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_alias(\n",
    "    collection_name=\"demo\",\n",
    "    alias=\"bob\"\n",
    ")\n",
    "\n",
    "client.create_alias(\n",
    "    collection_name=\"demo\",\n",
    "    alias=\"alice\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_collection(collection_name=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_id': False, 'description': '', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'enable_dynamic_field': True}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymilvus import DataType\n",
    "\n",
    "\n",
    "\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id = False,\n",
    "    enable_dynamic_field = True\n",
    ")\n",
    "\n",
    "\n",
    "schema.add_field(field_name=  \"id\", datatype= DataType.INT64, is_primary =True)\n",
    "schema.add_field(field_name=\"vector\",datatype= DataType.FLOAT_VECTOR, dim=768 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': <LoadState: NotExist>}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_load_state(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = MilvusClient.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\",\n",
    "    metric_type = \"COSINE\",\n",
    "    index_type=\"IVF_FLAT\",\n",
    "    index_name= \"vector_index\",\n",
    "    params = {\"nlsit\": 128}\n",
    ")\n",
    "\n",
    "client.create_index(\n",
    "    collection_name=\"demo\",\n",
    "    index_params=index_params,\n",
    "    sync = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vector_index']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_indexes(collection_name=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_type': 'IVF_FLAT',\n",
       " 'metric_type': 'COSINE',\n",
       " 'nlsit': '128',\n",
       " 'dim': '5',\n",
       " 'field_name': 'vector',\n",
       " 'index_name': 'vector_index',\n",
       " 'total_rows': 0,\n",
       " 'indexed_rows': 0,\n",
       " 'pending_index_rows': 0,\n",
       " 'state': 'Finished'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.describe_index(\n",
    "    collection_name=\"demo\",\n",
    "    index_name=\"vector_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 7500 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'subject'])\n",
      "Vector dim: 768\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "docs = [\n",
    "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "]\n",
    "docs = docs*2500\n",
    "vectors = [[random.uniform(-1, 1) for _ in range(768)] for _ in docs]\n",
    "data = [\n",
    "    {\"id\": i, \"vector\": vectors[i], \"text\": docs[i], \"subject\": \"history\"}\n",
    "    for i in range(len(vectors))\n",
    "]\n",
    "\n",
    "print(\"Data has\", len(data), \"entities, each with fields: \", data[0].keys())\n",
    "print(\"Vector dim:\", len(data[0][\"vector\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "index_params = MilvusClient.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", # Name of the vector field to be indexed\n",
    "    index_type=\"FLAT\", # Type of the index to create\n",
    "    index_name=\"vector_index01\", # Name of the index to create\n",
    "    metric_type=\"L2\", # Metric type used to measure similarity\n",
    "    params={} # No additional parameters required for FLAT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.has_collection(\"demo\"):\n",
    "    client.drop_collection(\"demo\")\n",
    "    \n",
    "client.create_collection(\n",
    "    collection_name=\"demo\",\n",
    "    index_params=index_params,\n",
    "    #schema= schema,\n",
    "    dimension=768,\n",
    "    metric_type=\"L2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= client.insert(\n",
    "    collection_name=\"demo\",\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [random.uniform(-1, 1) for _ in range(768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 ms, sys: 1.01 ms, total: 2.39 ms\n",
      "Wall time: 4.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = client.search(\n",
    "    collection_name = \"demo\",\n",
    "    data = [query],\n",
    "    limit =3,\n",
    "    #search_params={\"params\": {\"ef\":10}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [\"[{'id': 5744, 'distance': 443.8982849121094, 'entity': {}}, {'id': 932, 'distance': 445.388671875, 'entity': {}}, {'id': 1652, 'distance': 447.5111389160156, 'entity': {}}]\"]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vector']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_indexes(collection_name=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': '18',\n",
       " 'efConstruction': '240',\n",
       " 'index_type': 'AUTOINDEX',\n",
       " 'metric_type': 'L2',\n",
       " 'dim': '768',\n",
       " 'field_name': 'vector',\n",
       " 'index_name': 'vector',\n",
       " 'total_rows': 0,\n",
       " 'indexed_rows': 0,\n",
       " 'pending_index_rows': 0,\n",
       " 'state': 'Finished'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.describe_index(collection_name=\"demo\", index_name =\"vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_index(collection_name=\"demo\", index_name=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# async Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 16:54:40,941 [DEBUG][_create_connection]: Created new connection using: 0a46c69c731742d8aab1a24dd7737055 (async_milvus_client.py:600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auto_id': False, 'description': 'sample schema', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'dense_vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 5}}, {'name': 'sparse_vector', 'description': '', 'type': <DataType.SPARSE_FLOAT_VECTOR: 104>}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 512}}], 'enable_dynamic_field': False}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 16:52:23,419 [DEBUG][_create_connection]: Created new connection using: 044db53b444249eb97e730f036f45575 (async_milvus_client.py:600)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install \"pinecone[asyncio]\"\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from pinecone import PineconeAsyncio, ServerlessSpec\n",
    "\n",
    "async def main():\n",
    "    async with PineconeAsyncio(api_key=\"pcsk_345tD4_4PC4nTwKFbe2Vivjf3W9D9ZMGH9GjkqLu5XzKk1K2c8t9p2ZhDbeurFoj6ERgXw\") as pc:\n",
    "        if not await pc.has_index(\"example-index\"):\n",
    "            desc = await pc.create_index(\n",
    "                name=\"example-index\",\n",
    "                dimension=1536,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\"\n",
    "                ),\n",
    "                deletion_protection=\"disabled\",\n",
    "                tags={\n",
    "                    \"environment\": \"development\"\n",
    "                }\n",
    "            )\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeApiException",
     "evalue": "(409)\nReason: Conflict\nHTTP response headers: <CIMultiDictProxy('Content-Type': 'text/plain; charset=utf-8', 'Access-Control-Allow-Origin': '*', 'Vary': 'origin,access-control-request-method,access-control-request-headers', 'Access-Control-Expose-Headers': '*', 'x-pinecone-api-version': '2025-01', 'X-Cloud-Trace-Context': 'b5665b3366c3e041c5e3aa17356e0e11', 'Date': 'Thu, 20 Feb 2025 16:17:35 GMT', 'Server': 'Google Frontend', 'Content-Length': '85', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000')>\nHTTP response body: {\"error\":{\"code\":\"ALREADY_EXISTS\",\"message\":\"Resource  already exists\"},\"status\":409}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPineconeApiException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 79\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mIndexAsyncio(index_name\u001b[38;5;241m=\u001b[39midx_name) \u001b[38;5;28;01mas\u001b[39;00m idx:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mupsert_records(\n\u001b[1;32m     75\u001b[0m             namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m             records\u001b[38;5;241m=\u001b[39mrecords\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 79\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[118], line 59\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create index if it doesn't exist\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mhas_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://async-idx-auoio4m.svc.aped-4627-b74a.pinecone.io\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mcreate_index(\n\u001b[1;32m     60\u001b[0m         name\u001b[38;5;241m=\u001b[39midx_name,\n\u001b[1;32m     61\u001b[0m         spec\u001b[38;5;241m=\u001b[39mServerlessSpec(\n\u001b[1;32m     62\u001b[0m             cloud\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m             region\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-east-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         ),\n\u001b[1;32m     65\u001b[0m         dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     66\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Wait for index to be ready\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28;01mawait\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mdescribe_index(idx_name))\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mready:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/control/pinecone_asyncio.py:186\u001b[0m, in \u001b[0;36mPineconeAsyncio.create_index\u001b[0;34m(self, name, spec, dimension, metric, timeout, deletion_protection, vector_type, tags)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_index\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     tags: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IndexModel:\n\u001b[1;32m    177\u001b[0m     req \u001b[38;5;241m=\u001b[39m PineconeDBControlRequestFactory\u001b[38;5;241m.\u001b[39mcreate_index_request(\n\u001b[1;32m    178\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    179\u001b[0m         spec\u001b[38;5;241m=\u001b[39mspec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 186\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_api\u001b[38;5;241m.\u001b[39mcreate_index(create_index_request\u001b[38;5;241m=\u001b[39mreq)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m IndexModel(resp)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/asyncio_endpoint.py:90\u001b[0m, in \u001b[0;36mAsyncioEndpoint.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This method is invoked when endpoints are called\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/core/openapi/db_control/api/manage_indexes_api.py:905\u001b[0m, in \u001b[0;36mAsyncioManageIndexesApi.__init__.<locals>.__create_index\u001b[0;34m(self, create_index_request, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_openapi_kwargs(kwargs)\n\u001b[1;32m    904\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_index_request\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m create_index_request\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_with_http_info(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/asyncio_endpoint.py:122\u001b[0m, in \u001b[0;36mAsyncioEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m params \u001b[38;5;241m=\u001b[39m EndpointUtils\u001b[38;5;241m.\u001b[39mgather_params(\n\u001b[1;32m    113\u001b[0m     attribute_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_map,\n\u001b[1;32m    114\u001b[0m     location_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation_map,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    120\u001b[0m HeaderUtil\u001b[38;5;241m.\u001b[39mprepare_headers(headers_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders_map, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mcall_api(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint_path\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_method\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    125\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    126\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    127\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    128\u001b[0m     body\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    129\u001b[0m     post_params\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mform\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    130\u001b[0m     files\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    131\u001b[0m     response_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    132\u001b[0m     auth_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    133\u001b[0m     _check_type\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_check_return_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    134\u001b[0m     _return_http_data_only\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    135\u001b[0m     _preload_content\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_preload_content\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    136\u001b[0m     _request_timeout\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_request_timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    137\u001b[0m     _host\u001b[38;5;241m=\u001b[39m_host,\n\u001b[1;32m    138\u001b[0m     collection_formats\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_format\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    139\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/asyncio_api_client.py:258\u001b[0m, in \u001b[0;36mAsyncioApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_api\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    198\u001b[0m     resource_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     _check_type: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    214\u001b[0m ):\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    :param resource_path: Path to method endpoint.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    :type _check_type: bool, optional\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api(\n\u001b[1;32m    259\u001b[0m         resource_path,\n\u001b[1;32m    260\u001b[0m         method,\n\u001b[1;32m    261\u001b[0m         path_params,\n\u001b[1;32m    262\u001b[0m         query_params,\n\u001b[1;32m    263\u001b[0m         header_params,\n\u001b[1;32m    264\u001b[0m         body,\n\u001b[1;32m    265\u001b[0m         post_params,\n\u001b[1;32m    266\u001b[0m         files,\n\u001b[1;32m    267\u001b[0m         response_type,\n\u001b[1;32m    268\u001b[0m         auth_settings,\n\u001b[1;32m    269\u001b[0m         _return_http_data_only,\n\u001b[1;32m    270\u001b[0m         collection_formats,\n\u001b[1;32m    271\u001b[0m         _preload_content,\n\u001b[1;32m    272\u001b[0m         _request_timeout,\n\u001b[1;32m    273\u001b[0m         _host,\n\u001b[1;32m    274\u001b[0m         _check_type,\n\u001b[1;32m    275\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/asyncio_api_client.py:151\u001b[0m, in \u001b[0;36mAsyncioApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    150\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n\u001b[1;32m    155\u001b[0m return_data \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/asyncio_api_client.py:139\u001b[0m, in \u001b[0;36mAsyncioApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    130\u001b[0m url \u001b[38;5;241m=\u001b[39m build_request_url(\n\u001b[1;32m    131\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    132\u001b[0m     processed_path_params\u001b[38;5;241m=\u001b[39mprocessed_path_params,\n\u001b[1;32m    133\u001b[0m     resource_path\u001b[38;5;241m=\u001b[39mresource_path,\n\u001b[1;32m    134\u001b[0m     _host\u001b[38;5;241m=\u001b[39m_host,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    140\u001b[0m         method,\n\u001b[1;32m    141\u001b[0m         url,\n\u001b[1;32m    142\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mprocessed_query_params,\n\u001b[1;32m    143\u001b[0m         headers\u001b[38;5;241m=\u001b[39mprocessed_header_params,\n\u001b[1;32m    144\u001b[0m         post_params\u001b[38;5;241m=\u001b[39mprocessed_post_params,\n\u001b[1;32m    145\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    146\u001b[0m         _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    147\u001b[0m         _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    150\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/asyncio_api_client.py:316\u001b[0m, in \u001b[0;36mAsyncioApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(\n\u001b[1;32m    307\u001b[0m         url,\n\u001b[1;32m    308\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPOST(\n\u001b[1;32m    317\u001b[0m         url,\n\u001b[1;32m    318\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    319\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    320\u001b[0m         post_params\u001b[38;5;241m=\u001b[39mpost_params,\n\u001b[1;32m    321\u001b[0m         _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    322\u001b[0m         _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    323\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(\n\u001b[1;32m    327\u001b[0m         url,\n\u001b[1;32m    328\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    334\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/rest_aiohttp.py:142\u001b[0m, in \u001b[0;36mAiohttpRestClient.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mPOST\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m ):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    144\u001b[0m         url,\n\u001b[1;32m    145\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    146\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    147\u001b[0m         post_params\u001b[38;5;241m=\u001b[39mpost_params,\n\u001b[1;32m    148\u001b[0m         _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    149\u001b[0m         _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    150\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/rest_aiohttp.py:64\u001b[0m, in \u001b[0;36mAiohttpRestClient.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m     61\u001b[0m     method, url, params\u001b[38;5;241m=\u001b[39mquery_params, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mbody\n\u001b[1;32m     62\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m resp:\n\u001b[1;32m     63\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mraise_exceptions_or_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mRESTResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreason\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/openapi_support/rest_utils.py:49\u001b[0m, in \u001b[0;36mraise_exceptions_or_return\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m599\u001b[39m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mPineconeApiException\u001b[0m: (409)\nReason: Conflict\nHTTP response headers: <CIMultiDictProxy('Content-Type': 'text/plain; charset=utf-8', 'Access-Control-Allow-Origin': '*', 'Vary': 'origin,access-control-request-method,access-control-request-headers', 'Access-Control-Expose-Headers': '*', 'x-pinecone-api-version': '2025-01', 'X-Cloud-Trace-Context': 'b5665b3366c3e041c5e3aa17356e0e11', 'Date': 'Thu, 20 Feb 2025 16:17:35 GMT', 'Server': 'Google Frontend', 'Content-Length': '85', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000')>\nHTTP response body: {\"error\":{\"code\":\"ALREADY_EXISTS\",\"message\":\"Resource  already exists\"},\"status\":409}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from pinecone import PineconeAsyncio, ServerlessSpec\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_345tD4_4PC4nTwKFbe2Vivjf3W9D9ZMGH9GjkqLu5XzKk1K2c8t9p2ZhDbeurFoj6ERgXw\"\n",
    "async def main():\n",
    "    pc = PineconeAsyncio(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": \"1\",\n",
    "            \"title\": \"The Great Gatsby\",\n",
    "            \"author\": \"F. Scott Fitzgerald\",\n",
    "            \"description\": \"The story of the mysteriously wealthy Jay Gatsby and his love for the beautiful Daisy Buchanan.\",\n",
    "            \"year\": 1925,\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"2\",\n",
    "            \"title\": \"To Kill a Mockingbird\",\n",
    "            \"author\": \"Harper Lee\",\n",
    "            \"description\": \"A young girl comes of age in the segregated American South and witnesses her father's courageous defense of an innocent black man.\",\n",
    "            \"year\": 1960,\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"3\",\n",
    "            \"title\": \"1984\",\n",
    "            \"author\": \"George Orwell\",\n",
    "            \"description\": \"In a dystopian future, a totalitarian regime exercises absolute control through pervasive surveillance and propaganda.\",\n",
    "            \"year\": 1949,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = await pc.inference.embed(\n",
    "        model=\"multilingual-e5-large\",\n",
    "        inputs=[x[\"description\"] for x in data],\n",
    "        parameters={\n",
    "            \"input_type\": \"passage\",\n",
    "            \"truncate\": \"END\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Prepare records with correct metadata\n",
    "    records = []\n",
    "    for d, e in zip(data, embeddings):\n",
    "        records.append({\n",
    "            \"id\": d[\"id\"],\n",
    "            \"values\": e.values,  # Access embedding values correctly\n",
    "            \"metadata\": {\n",
    "                \"title\": d[\"title\"],  # Use actual title from data\n",
    "                \"author\": d[\"author\"],\n",
    "                \"year\": d[\"year\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    idx_name = \"async-idx\"\n",
    "    \n",
    "    # Create index if it doesn't exist\n",
    "    if not await pc.has_index(\"https://async-idx-auoio4m.svc.aped-4627-b74a.pinecone.io\"):\n",
    "        await pc.create_index(\n",
    "            name=idx_name,\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            ),\n",
    "            dimension=1024,\n",
    "            metric=\"cosine\"\n",
    "        )\n",
    "        # Wait for index to be ready\n",
    "        while not (await pc.describe_index(idx_name)).status.ready:\n",
    "            await asyncio.sleep(1)\n",
    "    \n",
    "    # Connect using index name (not hardcoded host)\n",
    "    async with pc.IndexAsyncio(index_name=idx_name) as idx:\n",
    "        await idx.upsert_records(\n",
    "            namespace=\"user0\",\n",
    "            records=records\n",
    "        )\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740070172.459653 1296402 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pymilvus 2.5.4 requires grpcio<=1.67.1,>=1.49.1, but you have grpcio 1.70.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"https://eec4f6d9-197f-4068-a99f-23bfaee0985f.eu-west-1-0.aws.cloud.qdrant.io\",\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIiwiZXhwIjoxNzQ3ODQ1OTczfQ.lbxybYBFRyeWqG_TWBXKo2muRASDgCihUpwOWAyBGgE\",\n",
    "    https= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseHandlingException",
     "evalue": "[Errno 54] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection.py:156\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_tls\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 156\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_tls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_backends/sync.py:154\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[0;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[1;32m    150\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    151\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    153\u001b[0m }\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:116\u001b[0m, in \u001b[0;36mApiClient.send_inner\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResponseHandlingException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqdrant_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Distance, VectorParams\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      5\u001b[0m    client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m      6\u001b[0m       collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m       vectors_config\u001b[38;5;241m=\u001b[39mVectorParams(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, distance\u001b[38;5;241m=\u001b[39mDistance\u001b[38;5;241m.\u001b[39mCOSINE),\n\u001b[1;32m      8\u001b[0m    )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/qdrant_client.py:2166\u001b[0m, in \u001b[0;36mQdrantClient.collection_exists\u001b[0;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check whether collection already exists\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m \n\u001b[1;32m   2158\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;124;03m    True if collection exists, False if not\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:2594\u001b[0m, in \u001b[0;36mQdrantRemote.collection_exists\u001b[0;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefer_grpc:\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrpc_collections\u001b[38;5;241m.\u001b[39mCollectionExists(\n\u001b[1;32m   2590\u001b[0m         grpc\u001b[38;5;241m.\u001b[39mCollectionExistsRequest(collection_name\u001b[38;5;241m=\u001b[39mcollection_name),\n\u001b[1;32m   2591\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m   2592\u001b[0m     )\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexists\n\u001b[0;32m-> 2594\u001b[0m result: Optional[models\u001b[38;5;241m.\u001b[39mCollectionExistence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection exists returned None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mexists\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api/collections_api.py:281\u001b[0m, in \u001b[0;36mSyncCollectionsApi.collection_exists\u001b[0;34m(self, collection_name)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollection_exists\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m m\u001b[38;5;241m.\u001b[39mInlineResponse2007:\n\u001b[1;32m    278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    Returns \\\"true\\\" if the given collection name exists, and \\\"false\\\" otherwise\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_for_collection_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api/collections_api.py:67\u001b[0m, in \u001b[0;36m_CollectionsApi._build_for_collection_exists\u001b[0;34m(self, collection_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m path_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(collection_name),\n\u001b[1;32m     64\u001b[0m }\n\u001b[1;32m     66\u001b[0m headers \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInlineResponse2007\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[38;5;124;43m/exists\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:89\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, type_, method, url, path_params, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbuild_request(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:106\u001b[0m, in \u001b[0;36mApiClient.send\u001b[0;34m(self, request, type_)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, type_: Type[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 106\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmiddleware\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m, \u001b[38;5;241m202\u001b[39m]:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:215\u001b[0m, in \u001b[0;36mBaseMiddleware.__call__\u001b[0;34m(self, request, call_next)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, call_next: Send) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:118\u001b[0m, in \u001b[0;36mApiClient.send_inner\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    116\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseHandlingException\u001b[0m: [Errno 54] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "if not client.collection_exists(\"my_collection\"):\n",
    "   client.create_collection(\n",
    "      collection_name=\"my_collection\",\n",
    "      vectors_config=VectorParams(size=100, distance=Distance.COSINE),\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseHandlingException",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_backends/sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    205\u001b[0m }\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    208\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    209\u001b[0m         address,\n\u001b[1;32m    210\u001b[0m         timeout,\n\u001b[1;32m    211\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    212\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:116\u001b[0m, in \u001b[0;36mApiClient.send_inner\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResponseHandlingException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqdrant_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorParams, Distance\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m    client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m      5\u001b[0m       collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m       vectors_config\u001b[38;5;241m=\u001b[39mVectorParams(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, distance\u001b[38;5;241m=\u001b[39mDistance\u001b[38;5;241m.\u001b[39mCOSINE),\n\u001b[1;32m      7\u001b[0m    )\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/qdrant_client.py:2166\u001b[0m, in \u001b[0;36mQdrantClient.collection_exists\u001b[0;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check whether collection already exists\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m \n\u001b[1;32m   2158\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;124;03m    True if collection exists, False if not\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:2594\u001b[0m, in \u001b[0;36mQdrantRemote.collection_exists\u001b[0;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefer_grpc:\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrpc_collections\u001b[38;5;241m.\u001b[39mCollectionExists(\n\u001b[1;32m   2590\u001b[0m         grpc\u001b[38;5;241m.\u001b[39mCollectionExistsRequest(collection_name\u001b[38;5;241m=\u001b[39mcollection_name),\n\u001b[1;32m   2591\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m   2592\u001b[0m     )\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexists\n\u001b[0;32m-> 2594\u001b[0m result: Optional[models\u001b[38;5;241m.\u001b[39mCollectionExistence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection exists returned None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mexists\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api/collections_api.py:281\u001b[0m, in \u001b[0;36mSyncCollectionsApi.collection_exists\u001b[0;34m(self, collection_name)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollection_exists\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m m\u001b[38;5;241m.\u001b[39mInlineResponse2007:\n\u001b[1;32m    278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    Returns \\\"true\\\" if the given collection name exists, and \\\"false\\\" otherwise\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_for_collection_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api/collections_api.py:67\u001b[0m, in \u001b[0;36m_CollectionsApi._build_for_collection_exists\u001b[0;34m(self, collection_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m path_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(collection_name),\n\u001b[1;32m     64\u001b[0m }\n\u001b[1;32m     66\u001b[0m headers \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInlineResponse2007\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[38;5;124;43m/exists\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:89\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, type_, method, url, path_params, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbuild_request(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:106\u001b[0m, in \u001b[0;36mApiClient.send\u001b[0;34m(self, request, type_)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, type_: Type[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 106\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmiddleware\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m, \u001b[38;5;241m202\u001b[39m]:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:215\u001b[0m, in \u001b[0;36mBaseMiddleware.__call__\u001b[0;34m(self, request, call_next)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, call_next: Send) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/qdrant_client/http/api_client.py:118\u001b[0m, in \u001b[0;36mApiClient.send_inner\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    116\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseHandlingException\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "if not client.collection_exists(\"my_collection\"):\n",
    "   client.create_collection(\n",
    "      collection_name=\"my_collection\",\n",
    "      vectors_config=VectorParams(size=100, distance=Distance.COSINE),\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740119674.787055 1296402 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "!pip install fastembed -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740119981.467532 1296402 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:03<00:00,  1.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "\n",
    "documents: List[str] = [\n",
    "    \"FastEmbed is lighter than Transformers & Sentence-Transformers.\",\n",
    "    \"FastEmbed is supported by and maintained by Qdrant.\",\n",
    "]\n",
    "\n",
    "\n",
    "embed_model = TextEmbedding()\n",
    "embeddings_generator = embed_model.embed(documents=documents,)\n",
    "\n",
    "embeddings_list = list(embeddings_generator)\n",
    "len(embeddings_list[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "client = QdrantClient(url = \"http://localhost:6333\")\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"payload\": {\n",
    "            \"chunk_part\": 0, \n",
    "            \"document_id\": \"a\"\n",
    "        },\n",
    "        \"vector\": [0.91]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"payload\": {\n",
    "            \"chunk_part\": 1, \n",
    "            \"document_id\": [\"a\", \"b\"]\n",
    "        },\n",
    "        \"vector\": [0.8]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"payload\": {\n",
    "            \"chunk_part\": 2, \n",
    "            \"document_id\": \"a\"\n",
    "        },\n",
    "        \"vector\": [0.2]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"payload\": {\n",
    "            \"chunk_part\": 0, \n",
    "            \"document_id\": 123\n",
    "        },\n",
    "        \"vector\": [0.79]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"payload\": {\n",
    "            \"chunk_part\": 1, \n",
    "            \"document_id\": 123\n",
    "        },\n",
    "        \"vector\": [0.75]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"payload\": {\n",
    "            \"chunk_part\": 0, \n",
    "            \"document_id\": -10\n",
    "        },\n",
    "        \"vector\": [0.6]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.models import  PointStruct\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=\"g_collection\",\n",
    "    points= [\n",
    "    PointStruct(id=item[\"id\"], vector=item[\"vector\"], payload=item[\"payload\"])\n",
    "    for item in data\n",
    "    ],\n",
    "    wait= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query_points_groups(\n",
    "    collection_name=\"g_collection\",\n",
    "    # Same as in the regular query_points() API\n",
    "    query=[1.1],\n",
    "    # Grouping parameters\n",
    "    group_by=\"id\",  # Path of the field to group by\n",
    "    limit=4,  # Max amount of groups\n",
    "    group_size=2,  # Max amount of points per group\n",
    ").groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "    collection_name=\"g_collection\",\n",
    "    vectors_config= VectorParams(size=4, distance= Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "num_points = 10000  # Number of dummy points to generate\n",
    "vector_size = 4  # Dimensionality of each vector\n",
    "\n",
    "# Generate dummy points\n",
    "points = []\n",
    "for i in range(num_points):\n",
    "    vector = np.random.rand(vector_size).tolist()  # Generate a random vector\n",
    "    payload = {\"attribute\": f\"value_{i}\"}  # Create a simple payload\n",
    "    point = PointStruct(id=i, vector=vector, payload=payload)\n",
    "    points.append(point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"g_collection\",\n",
    "    points= points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "client.create_payload_index(\n",
    "    collection_name=\"g_collection\",\n",
    "    field_name=\"attribute\",\n",
    "    field_schema=models.PayloadSchemaType.KEYWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a one-dimensional NumPy array\n",
    "array = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Convert the NumPy array to a list\n",
    "array_list = list(array)\n",
    "\n",
    "print(array_list)\n",
    "# Output: [1, 2, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.tolist() == list(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "df = pd.read_csv(\"rag_sample_qas_from_kis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ki_topic</th>\n",
       "      <th>ki_text</th>\n",
       "      <th>sample_question</th>\n",
       "      <th>sample_ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Setting Up a Mobile Device for Company Email</td>\n",
       "      <td>**Setting Up a Mobile Device for Company Email...</td>\n",
       "      <td>\"How do I set up my company email on my mobile...</td>\n",
       "      <td>To set up your company email on your mobile de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ki_topic  \\\n",
       "count                                             10   \n",
       "unique                                            10   \n",
       "top     Setting Up a Mobile Device for Company Email   \n",
       "freq                                               1   \n",
       "\n",
       "                                                  ki_text  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top     **Setting Up a Mobile Device for Company Email...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                          sample_question  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top     \"How do I set up my company email on my mobile...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                      sample_ground_truth  \n",
       "count                                                  10  \n",
       "unique                                                 10  \n",
       "top     To set up your company email on your mobile de...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = \"\\n\".join(df[\"ki_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.txt\", \"w\",encoding=\"utf-8\" ) as f:\n",
    "    f.write(combined_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(\"pcsk_345tD4_4PC4nTwKFbe2Vivjf3W9D9ZMGH9GjkqLu5XzKk1K2c8t9p2ZhDbeurFoj6ERgXw\")\n",
    "\n",
    "rerank_results = pc.inference.rerank(\n",
    "    model=\"bge-reranker-v2-m3\",\n",
    "    query=\"Tell me about the tech company known as Apple\",\n",
    "    documents=[\n",
    "      \"Apple is a popular fruit known for its sweetness and crisp texture.\",\t\n",
    "      \"Apple is known for its innovative products like the iPhone.\",\n",
    "      \"Many people enjoy eating apples as a healthy snack.\",\n",
    "      \"Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\",\n",
    "      \"An apple a day keeps the doctor away, as the saying goes.\",\n",
    "      \"apple just released their quterly result\",\n",
    "      \"apple tech pvt. ltd. notices the loss\",\n",
    "      \"aapple is the biggest laptop maufecturer in the entire moon\",\n",
    "      \"india suceesfully lanuchs it's new company named Apple\"\n",
    "    ],\n",
    "    top_n=10,\n",
    "    return_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RerankResult(\n",
       "  model='bge-reranker-v2-m3',\n",
       "  data=[{\n",
       "    index=1,\n",
       "    score=0.484685,\n",
       "    document={\n",
       "        text='Apple is known for its innovative products like the iPhone.'\n",
       "    }\n",
       "  },{\n",
       "    index=3,\n",
       "    score=0.3331777,\n",
       "    document={\n",
       "        text='Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.'\n",
       "    }\n",
       "  },{\n",
       "    index=8,\n",
       "    score=0.083291925,\n",
       "    document={\n",
       "        text=\"india suceesfully lanuchs it's new company named Apple\"\n",
       "    }\n",
       "  },{\n",
       "    index=7,\n",
       "    score=0.019419348,\n",
       "    document={\n",
       "        text='aapple is the biggest laptop maufecturer in the entire moon'\n",
       "    }\n",
       "  },{\n",
       "    index=0,\n",
       "    score=0.008577486,\n",
       "    document={\n",
       "        text='Apple is a popular fruit known for its sweetness and crisp texture.'\n",
       "    }\n",
       "  },{\n",
       "    index=6,\n",
       "    score=0.008156248,\n",
       "    document={\n",
       "        text='apple tech pvt. ltd. notices the loss'\n",
       "    }\n",
       "  },{\n",
       "    index=5,\n",
       "    score=0.0025509123,\n",
       "    document={\n",
       "        text='apple just released their quterly result'\n",
       "    }\n",
       "  },{\n",
       "    index=4,\n",
       "    score=0.0001465313,\n",
       "    document={\n",
       "        text='An apple a day keeps the doctor away, as the saying goes.'\n",
       "    }\n",
       "  },{\n",
       "    index=2,\n",
       "    score=1.6442495e-05,\n",
       "    document={\n",
       "        text='Many people enjoy eating apples as a healthy snack.'\n",
       "    }\n",
       "  }],\n",
       "  usage={'rerank_units': 1}\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.484685,\n",
       "  text='Apple is known for its innovative products like the iPhone.'),\n",
       " (0.3331777,\n",
       "  text='Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.'),\n",
       " (0.083291925, text=\"india suceesfully lanuchs it's new company named Apple\"),\n",
       " (0.019419348,\n",
       "  text='aapple is the biggest laptop maufecturer in the entire moon'),\n",
       " (0.008577486,\n",
       "  text='Apple is a popular fruit known for its sweetness and crisp texture.'),\n",
       " (0.008156248, text='apple tech pvt. ltd. notices the loss'),\n",
       " (0.0025509123, text='apple just released their quterly result'),\n",
       " (0.0001465313,\n",
       "  text='An apple a day keeps the doctor away, as the saying goes.'),\n",
       " (1.6442495e-05, text='Many people enjoy eating apples as a healthy snack.')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(entry[\"score\"], entry[\"document\"]) for entry in rerank_results.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RerankResult' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m], entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m rerank_results]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RerankResult' object is not iterable"
     ]
    }
   ],
   "source": [
    "[(entry[\"score\"], entry[\"document\"]) for entry in rerank_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_345tD4_4PC4nTwKFbe2Vivjf3W9D9ZMGH9GjkqLu5XzKk1K2c8t9p2ZhDbeurFoj6ERgXw\")\n",
    "\n",
    "data = [\n",
    "    {\"text\": \"The quick brown fox jumps over the lazy dog.\"},\n",
    "    {\"text\": \"The lazy dog is brown.\"},\n",
    "    {\"text\": \"The fox is brown.\"}\n",
    "]\n",
    "\n",
    "embeddings = pc.inference.embed(\n",
    "    model=\"pinecone-sparse-english-v0\",\n",
    "    inputs=[d['text'] for d in data],\n",
    "    parameters={\"input_type\": \"passage\", \"return_tokens\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingsList(\n",
      "  model='pinecone-sparse-english-v0',\n",
      "  vector_type='sparse',\n",
      "  data=[\n",
      "    {'vector_type': sparse, 'sparse_values': [3.9824219, 3.6289062, ..., 0.5703125, 4.5898438], 'sparse_indices': [741580288, 771291085, ..., 3162218338, 3422996809], 'sparse_tokens': ['brown', 'quick', ..., 'the', 'lazy']},\n",
      "    {'vector_type': sparse, 'sparse_values': [4.796875, 0.78222656, 4.5976562, 0.43359375, 5.7460938], 'sparse_indices': [741580288, 2021799277, 2982218203, 3162218338, 3422996809], 'sparse_tokens': ['brown', 'is', 'dog', 'the', 'lazy']},\n",
      "    {'vector_type': sparse, 'sparse_values': [4.9921875, 0.91796875, 5.5625, 0.5839844], 'sparse_indices': [741580288, 2021799277, 2673099881, 3162218338], 'sparse_tokens': ['brown', 'is', 'fox', 'the']}\n",
      "  ],\n",
      "  usage={'total_tokens': 27}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740225577.425547 1296402 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-text -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740225694.939184 1296402 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/krishgoyani/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 7210.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pinecone_text.sparse.bm25_encoder.BM25Encoder at 0x3404da260>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "corpus = [\"The quick brown fox jumps over the lazy dog\",\n",
    "        \"The lazy dog is brown\",\n",
    "        \"The fox is brown\"]\n",
    "\n",
    "bm25 = BM25Encoder()\n",
    "bm25 = BM25Encoder.default()\n",
    "bm25.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices': [226376294, 2982218203, 741580288],\n",
       " 'values': [0.4910714285714286, 0.4910714285714286, 0.4910714285714286]}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spares_vector_bm25 = bm25.encode_documents(\"The lazy dog is brown\")\n",
    "spares_vector_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sparse_vector = bm25.encode_queries(\"Which fox is brown?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices': [2673099881, 741580288],\n",
       " 'values': [0.7787512111381205, 0.2212487888618795]}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sparse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"sprase-dense-demo\",\n",
       "    \"metric\": \"dotproduct\",\n",
       "    \"host\": \"sprase-dense-demo-auoio4m.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 8,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "pc.create_index(\n",
    "    name=\"sprase-dense-demo\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    metric=\"dotproduct\",\n",
    "    dimension=8\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(name=\"sprase-dense-demo\")\n",
    "upsert_response = index.upsert(\n",
    "  vectors=[\n",
    "    {'id': 'vec1',\n",
    "      'values': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "      'sparse_values': {\n",
    "          'indices': [1, 5],\n",
    "          'values': [0.5, 0.5]\n",
    "      }},\n",
    "    {'id': 'vec2',\n",
    "      'values': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "      'sparse_values': {\n",
    "          'indices': [5, 6],\n",
    "          'values': [0.4, 0.5]\n",
    "      }}\n",
    "  ],\n",
    "  namespace='example-namespace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'vec2',\n",
       "              'score': 2.565,\n",
       "              'sparse_values': {'indices': [5, 6], 'values': [0.4, 0.5]},\n",
       "              'values': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n",
       "             {'id': 'vec1',\n",
       "              'score': 2.14,\n",
       "              'sparse_values': {'indices': [1, 5], 'values': [0.5, 0.5]},\n",
       "              'values': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}],\n",
       " 'namespace': 'example-namespace',\n",
       " 'usage': {'read_units': 11}}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\n",
    "    top_k=2,\n",
    "    vector = [0.2, 0.3, 0.4, 0.8, 0.6, 0.7, 0.85, 0.1],\n",
    "    namespace=\"example-namespace\",\n",
    "    include_values= True,\n",
    "    sparse_vector={\n",
    "          'indices': [5, 6],\n",
    "          'values': [0.45, 0.15]\n",
    "      }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qdrant_client -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QdrantClient.query_points() missing 1 required positional argument: 'collection_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqdrant_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QdrantClient, models\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m QdrantClient(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:6333\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: QdrantClient.query_points() missing 1 required positional argument: 'collection_name'"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "client.query_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone[grpc] in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (2025.1.31)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (2.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.66.0 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (1.67.0)\n",
      "Requirement already satisfied: grpcio>=1.44.0 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (1.70.0)\n",
      "Requirement already satisfied: lz4>=3.1.3 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (4.4.3)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.29 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (5.29.3)\n",
      "Requirement already satisfied: protoc-gen-openapiv2<0.0.2,>=0.0.1 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from pinecone[grpc]) (0.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/krishgoyani/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"pinecone[grpc]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = Pinecone(api_key=\"pcsk_345tD4_4PC4nTwKFbe2Vivjf3W9D9ZMGH9GjkqLu5XzKk1K2c8t9p2ZhDbeurFoj6ERgXw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone-text -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\n",
    "    \"The Renaissance was a period of European cultural, artistic, political, and economic revival following the Middle Ages. It began in Italy during the 14th century and spread across Europe, reaching its peak in the 16th century. This era is marked by the rediscovery of classical philosophy, literature, and art. Notable figures like Leonardo da Vinci and Michelangelo made significant contributions, influencing fields ranging from anatomy to engineering. The invention of the printing press by Johannes Gutenberg in the 15th century revolutionized information dissemination, leading to increased literacy and scientific progress.\",\n",
    "    \n",
    "    \"The theory of relativity, proposed by Albert Einstein in the early 20th century, transformed our understanding of space, time, and gravity. Special relativity, introduced in 1905, describes how the laws of physics remain constant for all observers in uniform motion. A key consequence is time dilation, where time slows down for objects moving at speeds close to the speed of light. General relativity, formulated in 1915, extends these principles to include gravity, explaining it as the curvature of spacetime caused by mass and energy. This theory has been confirmed through experiments like gravitational lensing and GPS satellite measurements.\",\n",
    "    \n",
    "    \"The Industrial Revolution, spanning from the late 18th to early 19th century, marked a shift from agrarian economies to industrialized societies. Innovations such as the steam engine, mechanized textile production, and improved iron manufacturing fueled rapid urbanization. Factories replaced traditional cottage industries, increasing production efficiency but also leading to harsh working conditions. The revolution had profound social and economic impacts, giving rise to capitalism and labor movements. Additionally, advancements in transportation, including railroads and steamships, facilitated global trade and the expansion of colonial empires.\",\n",
    "    \n",
    "    \"Quantum mechanics is a fundamental theory in physics describing the behavior of matter and energy at atomic and subatomic scales. Unlike classical mechanics, which follows deterministic laws, quantum mechanics introduces probabilistic principles. The wave-particle duality concept suggests that particles like electrons exhibit both wave-like and particle-like properties depending on the observation. The Heisenberg uncertainty principle states that one cannot simultaneously measure a particle’s position and momentum with absolute precision. Applications of quantum mechanics include semiconductors, lasers, and quantum computing, which leverages quantum superposition and entanglement to process information exponentially faster than classical computers.\",\n",
    "    \n",
    "    \"Artificial Intelligence (AI) encompasses a range of technologies aimed at simulating human intelligence in machines. AI can be categorized into narrow AI, designed for specific tasks like speech recognition, and general AI, which theoretically possesses human-like cognitive abilities. Machine learning, a subset of AI, enables systems to learn from data without explicit programming. Deep learning, powered by neural networks, has achieved breakthroughs in fields such as computer vision and natural language processing. AI applications span healthcare, finance, and autonomous systems, but ethical concerns regarding bias, job displacement, and AI safety remain topics of debate.\",\n",
    "    \n",
    "    \"Blockchain is a decentralized, distributed ledger technology that ensures secure and transparent record-keeping. It operates through a chain of blocks, each containing transaction data, which is verified by a consensus mechanism such as proof-of-work or proof-of-stake. Bitcoin, the first cryptocurrency, demonstrated blockchain’s potential for decentralized finance. Smart contracts, introduced by Ethereum, enable automated and self-executing agreements without intermediaries. Blockchain applications extend beyond finance to supply chain management, healthcare, and identity verification. Challenges such as scalability, energy consumption, and regulatory compliance continue to influence blockchain adoption.\",\n",
    "    \n",
    "    \"Cybersecurity involves protecting systems, networks, and data from cyber threats. It encompasses various domains, including network security, cryptography, and ethical hacking. Attack vectors such as malware, phishing, and denial-of-service attacks exploit vulnerabilities to gain unauthorized access. Organizations employ firewalls, encryption, and multi-factor authentication to enhance security. With the rise of cloud computing and IoT devices, cybersecurity risks have increased. Governments and industries emphasize cybersecurity frameworks like NIST and GDPR to safeguard digital assets. The future of cybersecurity includes AI-driven threat detection and quantum-resistant cryptographic algorithms.\",\n",
    "    \n",
    "    \"The theory of evolution, formulated by Charles Darwin in 'On the Origin of Species' (1859), explains the diversity of life through natural selection. Organisms with advantageous traits are more likely to survive and reproduce, passing these traits to future generations. Over millions of years, this process leads to the emergence of new species. Fossil records, genetic evidence, and comparative anatomy support evolutionary theory. The modern synthesis integrates Darwin’s principles with genetics, highlighting the role of mutations and genetic drift in evolution. Evolutionary biology influences fields like medicine, conservation, and artificial life simulations.\",\n",
    "    \n",
    "    \"Neuroscience explores the structure and function of the nervous system, particularly the brain. Neurons, the basic units of the brain, transmit information via electrical and chemical signals. The brain consists of regions like the cerebral cortex, responsible for cognition, and the limbic system, which regulates emotions. Neuroplasticity, the brain’s ability to rewire itself, plays a crucial role in learning and recovery from injuries. Advanced techniques such as fMRI and EEG help researchers understand brain activity. Neuroscience intersects with psychology, artificial intelligence, and cognitive science, influencing areas like brain-computer interfaces and mental health treatments.\",\n",
    "    \n",
    "    \"... (Continue similarly for 10 more diverse topics, each forming a chunk of ~500 tokens) ...\"\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    \"How did the Renaissance impact European society?\",\n",
    "    \"What are the key principles of Einstein's theory of relativity?\",\n",
    "    \"How did the Industrial Revolution change the global economy?\",\n",
    "    \"Explain the role of quantum mechanics in modern technology.\",\n",
    "    \"What are the ethical concerns related to Artificial Intelligence?\",\n",
    "    \"How does blockchain technology ensure security in transactions?\",\n",
    "    \"What are the major cybersecurity threats faced by organizations?\",\n",
    "    \"How does Darwin's theory of evolution explain species adaptation?\",\n",
    "    \"What is neuroplasticity and how does it affect learning?\",\n",
    "    \"... (Include more queries covering different chunks) ...\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 248.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pinecone_text.sparse.bm25_encoder.BM25Encoder at 0x30265be80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "bm25 = BM25Encoder()\n",
    "bm25.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BM25Encoder.encode_documents() missing 1 required positional argument: 'texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: BM25Encoder.encode_documents() missing 1 required positional argument: 'texts'"
     ]
    }
   ],
   "source": [
    "bm25.encode_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_emebeddings = []\n",
    "for x in chunks:\n",
    "    sparse_emebeddings.append(bm25.encode_documents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector(\n",
    "            id=\"vec1\",\n",
    "            sparse_values=SparseValues(\n",
    "                values=[1.7958984, 0.41577148, 2.828125, 2.8027344, 2.8691406, 1.6533203, 5.3671875, 1.3046875, 0.49780273, 0.5722656, 2.71875, 3.0820312, 2.5019531, 4.4414062, 3.3554688],\n",
    "                indices=[822745112, 1009084850, 1221765879, 1408993854, 1504846510, 1596856843, 1640781426, 1656251611, 1807131503, 2543655733, 2902766088, 2909307736, 3246437992, 3517203014, 3590924191]\n",
    "            ),\n",
    "            metadata={\n",
    "                \"source_text\": \"AAPL reported a year-over-year revenue increase, expecting stronger Q3 demand for its flagship phones.\",\n",
    "                \"category\": \"technology\",\n",
    "                \"quarter\": \"Q3\"\n",
    "            }\n",
    "        ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"sparse-0\",\n",
       "    \"metric\": \"dotproduct\",\n",
       "    \"host\": \"sparse-0-auoio4m.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"sparse\",\n",
       "    \"dimension\": null,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "pc.create_index(\n",
    "    name=\"sparse-0\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    metric=\"dotproduct\",\n",
    "    vector_type=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, SparseValues, Vector\n",
    "\n",
    "import uuid\n",
    "vectors = []\n",
    "for d,e in zip(chunks, sparse_emebeddings):\n",
    "    vec = Vector(\n",
    "        id= str(uuid.uuid4()),\n",
    "        sparse_values=SparseValues(\n",
    "            values=e[\"values\"],\n",
    "            indices=e[\"indices\"]\n",
    "        ),\n",
    "        metadata={\"source_text\": d}\n",
    "    )\n",
    "    vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_embeddings = pc.inference.embed(\n",
    "    model=\"llama-text-embed-v2\",\n",
    "    inputs= chunks,\n",
    "    parameters={\"input_type\": \"passage\", \"truncate\": \"END\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 10}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "records =[]\n",
    "for d,e in zip(chunks,dense_embeddings):\n",
    "    records.append({\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"values\": e['values'],\n",
    "        \"metadata\": {'text': d}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"dense-0\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"dense-0-auoio4m.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 1024,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.create_index(\n",
    "    name=\"dense-0\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    vector_type=\"dense\",\n",
    "    dimension=1024,\n",
    "    metric=\"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upserted_count: 10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_index = pc.Index(name=\"dense-0\")\n",
    "\n",
    "d_index.upsert(\n",
    "    vectors= records\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'c6dc7210-f481-4b3d-95b3-230580aa27ff',\n",
       "              'metadata': {'source_text': 'The Renaissance was a period of '\n",
       "                                          'European cultural, artistic, '\n",
       "                                          'political, and economic revival '\n",
       "                                          'following the Middle Ages. It began '\n",
       "                                          'in Italy during the 14th century '\n",
       "                                          'and spread across Europe, reaching '\n",
       "                                          'its peak in the 16th century. This '\n",
       "                                          'era is marked by the rediscovery of '\n",
       "                                          'classical philosophy, literature, '\n",
       "                                          'and art. Notable figures like '\n",
       "                                          'Leonardo da Vinci and Michelangelo '\n",
       "                                          'made significant contributions, '\n",
       "                                          'influencing fields ranging from '\n",
       "                                          'anatomy to engineering. The '\n",
       "                                          'invention of the printing press by '\n",
       "                                          'Johannes Gutenberg in the 15th '\n",
       "                                          'century revolutionized information '\n",
       "                                          'dissemination, leading to increased '\n",
       "                                          'literacy and scientific progress.'},\n",
       "              'score': 0.225680932,\n",
       "              'values': []},\n",
       "             {'id': 'e2024f7b-f664-4bd5-be62-9ae8ee12b16a',\n",
       "              'metadata': {'source_text': 'The Industrial Revolution, spanning '\n",
       "                                          'from the late 18th to early 19th '\n",
       "                                          'century, marked a shift from '\n",
       "                                          'agrarian economies to '\n",
       "                                          'industrialized societies. '\n",
       "                                          'Innovations such as the steam '\n",
       "                                          'engine, mechanized textile '\n",
       "                                          'production, and improved iron '\n",
       "                                          'manufacturing fueled rapid '\n",
       "                                          'urbanization. Factories replaced '\n",
       "                                          'traditional cottage industries, '\n",
       "                                          'increasing production efficiency '\n",
       "                                          'but also leading to harsh working '\n",
       "                                          'conditions. The revolution had '\n",
       "                                          'profound social and economic '\n",
       "                                          'impacts, giving rise to capitalism '\n",
       "                                          'and labor movements. Additionally, '\n",
       "                                          'advancements in transportation, '\n",
       "                                          'including railroads and steamships, '\n",
       "                                          'facilitated global trade and the '\n",
       "                                          'expansion of colonial empires.'},\n",
       "              'score': 0.222563311,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 1}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s_results = index.query(\n",
    "    sparse_vector= bm25.encode_queries(queries[0]),\n",
    "    top_k=5,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")\n",
    "s_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeException",
     "evalue": "UNKNOWN:Error received from peer  {grpc_message:\"Unauthorized\", grpc_status:16, created_time:\"2025-02-24T16:38:07.357927+05:30\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/grpc/grpc_runner.py:43\u001b[0m, in \u001b[0;36mGrpcRunner.run.<locals>.wrapped\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _InactiveRpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAUTHENTICATED\n\tdetails = \"Unauthorized\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Unauthorized\", grpc_status:16, created_time:\"2025-02-24T16:38:07.357927+05:30\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPineconeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39minference\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-text-embed-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[queries[\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     }\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m d_results \u001b[38;5;241m=\u001b[39m \u001b[43md_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/grpc/index_grpc.py:412\u001b[0m, in \u001b[0;36mGRPCIndex.query\u001b[0;34m(self, vector, id, namespace, top_k, filter, include_values, include_metadata, sparse_vector, async_req, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PineconeGrpcFuture(future)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     json_response \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(response)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_query_response(json_response, _check_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/grpc/grpc_runner.py:54\u001b[0m, in \u001b[0;36mGrpcRunner.run\u001b[0;34m(self, func, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m _InactiveRpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PineconeException(e\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdebug_error_string) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Data Ingestion and RAG/env/lib/python3.10/site-packages/pinecone/grpc/grpc_runner.py:52\u001b[0m, in \u001b[0;36mGrpcRunner.run.<locals>.wrapped\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[1;32m     44\u001b[0m         request,\n\u001b[1;32m     45\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _InactiveRpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeException(e\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdebug_error_string) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mPineconeException\u001b[0m: UNKNOWN:Error received from peer  {grpc_message:\"Unauthorized\", grpc_status:16, created_time:\"2025-02-24T16:38:07.357927+05:30\"}"
     ]
    }
   ],
   "source": [
    "query_embedding = pc.inference.embed(\n",
    "    model=\"llama-text-embed-v2\",\n",
    "    inputs=[queries[0]],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "d_results = d_index.query(\n",
    "    vector= query_embedding[0].values,\n",
    "    top_k=5,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding.usage[\"total_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def reciprocal_rank_fusion(*list_of_list_ranks_system, K=60):\n",
    "    \"\"\"\n",
    "    Fuse rank from multiple IR systems using Reciprocal Rank Fusion.\n",
    "    \n",
    "    Args:\n",
    "    * list_of_list_ranks_system: Ranked results from different IR system.\n",
    "    K (int): A constant used in the RRF formula (default is 60).\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of list of sorted documents by score and sorted documents\n",
    "    \"\"\"\n",
    "    # Dictionary to store RRF mapping\n",
    "    rrf_map = defaultdict(float)\n",
    "\n",
    "    # Calculate RRF score for each result in each list\n",
    "    for rank_list in list_of_list_ranks_system:\n",
    "        for rank, item in enumerate(rank_list, 1):\n",
    "            rrf_map[item] += 1 / (rank + K)\n",
    "\n",
    "    # Sort items based on their RRF scores in descending order\n",
    "    sorted_items = sorted(rrf_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return tuple of list of sorted documents by score and sorted documents\n",
    "    return sorted_items, [item for item, score in sorted_items]\n",
    "ense_results = [x[\"metadata\"][\"text\"] for x in d_results[\"matches\"]]  # e.g., result from your dense index query\n",
    "sparse_results = [x[\"metadata\"][\"source_text\"] for x in s_results[\"matches\"]]\n",
    "\n",
    "# Example ranked lists from different sources\n",
    "ir_system_a = ense_results\n",
    "ir_system_b = sparse_results\n",
    "\n",
    "\n",
    "# Combine the lists using RRF\n",
    "combined_list = reciprocal_rank_fusion(ir_system_a, ir_system_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Renaissance was a period of European cultural, artistic, political, and economic revival following the Middle Ages. It began in Italy during the 14th century and spread across Europe, reaching its peak in the 16th century. This era is marked by the rediscovery of classical philosophy, literature, and art. Notable figures like Leonardo da Vinci and Michelangelo made significant contributions, influencing fields ranging from anatomy to engineering. The invention of the printing press by Johannes Gutenberg in the 15th century revolutionized information dissemination, leading to increased literacy and scientific progress.',\n",
       "  0.03278688524590164),\n",
       " ('The Industrial Revolution, spanning from the late 18th to early 19th century, marked a shift from agrarian economies to industrialized societies. Innovations such as the steam engine, mechanized textile production, and improved iron manufacturing fueled rapid urbanization. Factories replaced traditional cottage industries, increasing production efficiency but also leading to harsh working conditions. The revolution had profound social and economic impacts, giving rise to capitalism and labor movements. Additionally, advancements in transportation, including railroads and steamships, facilitated global trade and the expansion of colonial empires.',\n",
       "  0.03225806451612903),\n",
       " (\"The theory of evolution, formulated by Charles Darwin in 'On the Origin of Species' (1859), explains the diversity of life through natural selection. Organisms with advantageous traits are more likely to survive and reproduce, passing these traits to future generations. Over millions of years, this process leads to the emergence of new species. Fossil records, genetic evidence, and comparative anatomy support evolutionary theory. The modern synthesis integrates Darwin’s principles with genetics, highlighting the role of mutations and genetic drift in evolution. Evolutionary biology influences fields like medicine, conservation, and artificial life simulations.\",\n",
       "  0.015873015873015872),\n",
       " ('The theory of relativity, proposed by Albert Einstein in the early 20th century, transformed our understanding of space, time, and gravity. Special relativity, introduced in 1905, describes how the laws of physics remain constant for all observers in uniform motion. A key consequence is time dilation, where time slows down for objects moving at speeds close to the speed of light. General relativity, formulated in 1915, extends these principles to include gravity, explaining it as the curvature of spacetime caused by mass and energy. This theory has been confirmed through experiments like gravitational lensing and GPS satellite measurements.',\n",
       "  0.015625),\n",
       " ('Neuroscience explores the structure and function of the nervous system, particularly the brain. Neurons, the basic units of the brain, transmit information via electrical and chemical signals. The brain consists of regions like the cerebral cortex, responsible for cognition, and the limbic system, which regulates emotions. Neuroplasticity, the brain’s ability to rewire itself, plays a crucial role in learning and recovery from injuries. Advanced techniques such as fMRI and EEG help researchers understand brain activity. Neuroscience intersects with psychology, artificial intelligence, and cognitive science, influencing areas like brain-computer interfaces and mental health treatments.',\n",
       "  0.015384615384615385)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Renaissance was a period of European cultural, artistic, political, and economic revival following the Middle Ages. It began in Italy during the 14th century and spread across Europe, reaching its peak in the 16th century. This era is marked by the rediscovery of classical philosophy, literature, and art. Notable figures like Leonardo da Vinci and Michelangelo made significant contributions, influencing fields ranging from anatomy to engineering. The invention of the printing press by Johannes Gutenberg in the 15th century revolutionized information dissemination, leading to increased literacy and scientific progress.',\n",
       " 'The Industrial Revolution, spanning from the late 18th to early 19th century, marked a shift from agrarian economies to industrialized societies. Innovations such as the steam engine, mechanized textile production, and improved iron manufacturing fueled rapid urbanization. Factories replaced traditional cottage industries, increasing production efficiency but also leading to harsh working conditions. The revolution had profound social and economic impacts, giving rise to capitalism and labor movements. Additionally, advancements in transportation, including railroads and steamships, facilitated global trade and the expansion of colonial empires.',\n",
       " \"The theory of evolution, formulated by Charles Darwin in 'On the Origin of Species' (1859), explains the diversity of life through natural selection. Organisms with advantageous traits are more likely to survive and reproduce, passing these traits to future generations. Over millions of years, this process leads to the emergence of new species. Fossil records, genetic evidence, and comparative anatomy support evolutionary theory. The modern synthesis integrates Darwin’s principles with genetics, highlighting the role of mutations and genetic drift in evolution. Evolutionary biology influences fields like medicine, conservation, and artificial life simulations.\",\n",
       " 'The theory of relativity, proposed by Albert Einstein in the early 20th century, transformed our understanding of space, time, and gravity. Special relativity, introduced in 1905, describes how the laws of physics remain constant for all observers in uniform motion. A key consequence is time dilation, where time slows down for objects moving at speeds close to the speed of light. General relativity, formulated in 1915, extends these principles to include gravity, explaining it as the curvature of spacetime caused by mass and energy. This theory has been confirmed through experiments like gravitational lensing and GPS satellite measurements.',\n",
       " 'Neuroscience explores the structure and function of the nervous system, particularly the brain. Neurons, the basic units of the brain, transmit information via electrical and chemical signals. The brain consists of regions like the cerebral cortex, responsible for cognition, and the limbic system, which regulates emotions. Neuroplasticity, the brain’s ability to rewire itself, plays a crucial role in learning and recovery from injuries. Advanced techniques such as fMRI and EEG help researchers understand brain activity. Neuroscience intersects with psychology, artificial intelligence, and cognitive science, influencing areas like brain-computer interfaces and mental health treatments.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[\"metadata\"][\"text\"] for x in d_results[\"matches\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "\n",
    "    #\n",
    "    # Optional parameters\n",
    "    #\n",
    "\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 32,768 tokens shared between prompt and completion.\n",
    "    max_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=1,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-d4fbc992-88ee-4a4e-a5ce-f2f9d463330c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Fast language models are crucial in today's world, and their importance can be seen in several areas:\\n\\n1. **Efficient Processing**: Fast language models can process and analyze vast amounts of text data quickly, making them ideal for applications that require real-time or near-real-time processing, such as chatbots, virtual assistants, and language translation software.\\n2. **Improved User Experience**: Faster language models enable more responsive and interactive interfaces, which can lead to a better user experience. For example, a fast language model can quickly generate responses to user queries, making the interaction feel more natural and seamless.\\n3. **Increased Productivity**: By automating tasks such as text summarization, sentiment analysis, and language translation, fast language models can help increase productivity and reduce the workload of human workers.\\n4. **Enhanced Decision-Making**: Fast language models can quickly analyze large amounts of text data, providing valuable insights and patterns that can inform decision-making in various industries, such as finance, healthcare, and marketing.\\n5. **Real-Time Analytics**: Fast language models can analyze text data in real-time, enabling organizations to monitor and respond to trends, sentiment, and other factors that can impact their business or operations.\\n6. **Competitive Advantage**: Companies that adopt fast language models can gain a competitive advantage by being able to respond quickly to changing market conditions, customer needs, and other factors that can impact their business.\\n7. **Scalability**: Fast language models can handle large volumes of text data, making them suitable for applications that require processing and analyzing vast amounts of data, such as social media monitoring, customer feedback analysis, and content moderation.\\n8. **Cost Savings**: By automating tasks and processes, fast language models can help reduce labor costs and improve operational efficiency, leading to cost savings for organizations.\\n9. **Improved Accuracy**: Fast language models can be trained on large datasets, which can lead to improved accuracy and reduced errors in tasks such as language translation, text classification, and sentiment analysis.\\n10. **Innovation and Research**: Fast language models can accelerate innovation and research in areas such as natural language processing, machine learning, and artificial intelligence, leading to new breakthroughs and discoveries.\\n\\nSome examples of applications that benefit from fast language models include:\\n\\n* Virtual assistants (e.g., Siri, Alexa, Google Assistant)\\n* Chatbots and customer service platforms\\n* Language translation software (e.g., Google Translate)\\n* Text summarization and sentiment analysis tools\\n* Social media monitoring and analytics platforms\\n* Content moderation and filtering systems\\n* Speech recognition and voice-to-text systems\\n\\nOverall, fast language models have the potential to revolutionize the way we interact with text data, enabling faster, more accurate, and more efficient processing and analysis of large amounts of information.\", role='assistant', function_call=None, tool_calls=None))], created=1740393918, model='llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ca0059abb', usage=CompletionUsage(completion_tokens=557, prompt_tokens=49, total_tokens=606, completion_time=2.025454545, prompt_time=0.005915292, queue_time=0.36283049, total_time=2.031369837), x_groq={'id': 'req_01jmvrzbvffexrzfapk9385grp'})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  await self.file_conversion_service.convert_to_text(file_bytes)\n",
    "        chunks =  self.text_splitter.recursive_text_splitter(text, chunk_size, chunk_overlap)\n",
    "        await self.vector_db_service.pinecone_generate_and_store_embeddings(chunks)\n",
    "        pinecone_chunks = await self.retrieve_chunks_service.pinecone_retrieve_similar_chunks(query, 10)\n",
    "        response  = await self.llm_response_service.generate_response(retrieved_chunks, query)\n",
    "        dense_embeddings = await self.embedding_service.generate_embeddings(chunks)\n",
    "        sparse_embeddings = self.sprase_embedding_service.generate_sparse_embeddings(chunks)\n",
    "        await self.vector_db_service.pinecone_store_sparse_embeddings(chunks, sparse_embeddings)\n",
    "        sparse_chunks = await self.retrieve_chunks_service.pinecone_retrieve_similar_chunks_s(query, 10)\n",
    "        sorted_items, sorted_documents = self.rrf_service.fuse(pinecone_chunks, sparse_chunks)\n",
    "        final_chunks = await self.re_ranking_service.re_ranker(query, sorted_documents)\n",
    "        return final_chunks, pinecone_chunks, sparse_chunks\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
